{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJPJNXTJuVsS",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "# Initial imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ztlz1_Hl4YM5"
      },
      "outputs": [],
      "source": [
        "# to import files from googledrive\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbmC5P29d1vW"
      },
      "outputs": [],
      "source": [
        "# check version of python\n",
        "\n",
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "install and import required packages"
      ],
      "metadata": {
        "id": "uWAemzjaGrtl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvgUZ_TVd1vW"
      },
      "outputs": [],
      "source": [
        "!pip install -U scikit-learn==1.2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csnZDsE7d1vX"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "sklearn.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfZffjko4dHn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSGW-nh5-GfE",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "# Data for Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1qBDOWMRUGm"
      },
      "source": [
        "Load the data for classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAWygjHuhqzT"
      },
      "outputs": [],
      "source": [
        "path = './'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2XLH1_R4eWj"
      },
      "outputs": [],
      "source": [
        "data_for_classifier = pd.read_csv(path+'gold_std_clean.csv')\n",
        "#data_for_classifier = pd.read_csv(path+'gold_std_for_anatomy.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "take a look at first 5 rows of the data and drop columns not required, or rename as needed"
      ],
      "metadata": {
        "id": "cp6VAUW5GxHa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwGUExRK4kis"
      },
      "outputs": [],
      "source": [
        "data_for_classifier.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGQeqY9sd1vd"
      },
      "outputs": [],
      "source": [
        "data_for_classifier = data_for_classifier.drop(columns=['some cells have vals for 5th round vlookup because docid have altered post medcat', 'pain_character', 'pain_management', 'self_harm'  ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3-RysUtd1ve"
      },
      "outputs": [],
      "source": [
        "data_for_classifier['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hIxFdXCd1vf"
      },
      "outputs": [],
      "source": [
        "#ignore --- convert labels (relevant Yes = 1, No = 0, Negated = 0)\n",
        "\n",
        "data_for_classifier['relevant'] = np.where(data_for_classifier['relevant'] == \"Yes\", int(1), data_for_classifier['relevant'])\n",
        "data_for_classifier['relevant'] = np.where(data_for_classifier['relevant'] == \"yes\", int(1), data_for_classifier['relevant'])\n",
        "data_for_classifier['relevant'] = np.where(data_for_classifier['relevant'] == \"YES\", int(1), data_for_classifier['relevant'])\n",
        "data_for_classifier['relevant'] = np.where(data_for_classifier['relevant'] == \"No\", int(0), data_for_classifier['relevant'])\n",
        "data_for_classifier['relevant'] = np.where(data_for_classifier['relevant'] == \"no\", int(0), data_for_classifier['relevant'])\n",
        "data_for_classifier['relevant'] = np.where(data_for_classifier['relevant'] == \"Negated\", int(0), data_for_classifier['relevant'])\n",
        "data_for_classifier['relevant'] = np.where(data_for_classifier['relevant'] == \"negated\", int(0), data_for_classifier['relevant'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IK_kUXA4l33"
      },
      "outputs": [],
      "source": [
        "data_for_classifier.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfXRHdXWd1vg"
      },
      "outputs": [],
      "source": [
        "#ignore --- rename columns\n",
        "\n",
        "data_for_classifier.rename(columns={'relevant': 'label'}, inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTJf8cu5RXqu",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "# Split the data into train and test (80:20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5QaX57tGOT4"
      },
      "outputs": [],
      "source": [
        "training_data, testing_data = train_test_split(data_for_classifier, test_size=0.2, random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vB36IF7GULX"
      },
      "outputs": [],
      "source": [
        "print(f\"No. of training examples: {training_data.shape[0]}\")\n",
        "print(f\"No. of testing examples: {testing_data.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGjrekCqGWyL"
      },
      "outputs": [],
      "source": [
        "training_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IwL0UoZGYpX"
      },
      "outputs": [],
      "source": [
        "training_data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y113XixbGitL"
      },
      "outputs": [],
      "source": [
        "training_data['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqrL8YRlGacf"
      },
      "outputs": [],
      "source": [
        "testing_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTLDHB56d1vi"
      },
      "outputs": [],
      "source": [
        "testing_data.to_csv('test_data_pain.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tET_RMKDGepz"
      },
      "outputs": [],
      "source": [
        "testing_data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxrZBSBgGgBV"
      },
      "outputs": [],
      "source": [
        "testing_data['label'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klq3iD_y-K43",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "# BERT Classification 1\n",
        "\n",
        "(Run the second BERT classification below for confidence intervals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSW39FULGy2P"
      },
      "source": [
        "BERT classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install and import pacakges needed"
      ],
      "metadata": {
        "id": "5kVyFgOjG7Qp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gh0O7j9Cd1vi"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3SpcqUfwjBs"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSpe3FIAd1vj"
      },
      "outputs": [],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnivD2i-wlkd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXy9E_HiGybM"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.5.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RA1zt3pKd1vj"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DpmjFihGplE"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import TensorDataset\n",
        "from tqdm import trange\n",
        "from transformers import BertForSequenceClassification, BertTokenizerFast, BertTokenizer, AdamW, AutoModel, AutoModelForSequenceClassification, AutoTokenizer, AutoModelForMaskedLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4MjYo-2d1vk"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6j5SH__d1vk"
      },
      "source": [
        "FOR bert_base_uncased"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHhbX1FI_uWg"
      },
      "outputs": [],
      "source": [
        "BERT_tokenizer = 'bert-base-uncased'\n",
        "#BERT_tokenizer = 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext'\n",
        "MAX_TKN_LEN=511"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJi5tQ4lHKx5"
      },
      "outputs": [],
      "source": [
        "# import BERT-base pretrained model\n",
        "#trained_bert_model = AutoModel.from_pretrained('bert-base-uncased')\n",
        "#trained_bert_model = AutoModelForSequenceClassification.from_pretrained(BERT_tokenizer, num_labels = 2, output_attentions = False, output_hidden_states = False)\n",
        "#trained_bert_model = AutoModel.from_pretrained(BERT_tokenizer, num_labels = 2, output_attentions = False, output_hidden_states = False)\n",
        "trained_bert_model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "trained_bert_model.cuda()\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "#tokenizer = AutoTokenizer.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly7SAHTJd1vl"
      },
      "source": [
        "FOR SAPBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJx-krKWd1vl"
      },
      "outputs": [],
      "source": [
        "#BERT_tokenizer = 'bert-base-uncased'\n",
        "BERT_tokenizer = 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext'\n",
        "MAX_TKN_LEN=511"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HrjeQMvd1vl"
      },
      "outputs": [],
      "source": [
        "# import BERT-base pretrained model\n",
        "#trained_bert_model = AutoModel.from_pretrained('bert-base-uncased')\n",
        "#trained_bert_model = AutoModelForSequenceClassification.from_pretrained(BERT_tokenizer, num_labels = 2, output_attentions = False, output_hidden_states = False)\n",
        "#trained_bert_model = AutoModel.from_pretrained(BERT_tokenizer, num_labels = 2, output_attentions = False, output_hidden_states = False)\n",
        "#trained_bert_model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "trained_bert_model = AutoModel.from_pretrained(\"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\")\n",
        "trained_bert_model.cuda()\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "#tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "tokenizer = AutoTokenizer.from_pretrained('cambridgeltl/SapBERT-from-PubMedBERT-fulltext')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training data"
      ],
      "metadata": {
        "id": "xzg2IC5NHEyj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fKQKL7JAVPq"
      },
      "outputs": [],
      "source": [
        "training_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5tQL_sjAXe4"
      },
      "outputs": [],
      "source": [
        "#ifnore if you don't have these columns already\n",
        "training_data = training_data.drop(columns=['task','word_count', 'char_count', 'unique_word_count'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_hxuuUeCfpj"
      },
      "outputs": [],
      "source": [
        "training_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdzUrGukDc8l"
      },
      "outputs": [],
      "source": [
        "text_col = 'context'\n",
        "label_col = 'label'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEYZqha9HQo2"
      },
      "outputs": [],
      "source": [
        "training_data['context'] = training_data['context'].apply(lambda x: x.strip().lower())\n",
        "training_data['context']= training_data['context'].str.replace('[^\\w\\s]','')\n",
        "print('num annotations:', len(training_data), '\\n\\n', training_data['label'].value_counts(), '\\n\\n', training_data[['label', 'context']].head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "8Di-vYpJd1vm"
      },
      "source": [
        "# BERT functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCVZBCk7EdBo"
      },
      "outputs": [],
      "source": [
        "# Create the DataLoaders for our training and validation sets.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32.\n",
        "def create_dataloader(train_dataset, val_dataset, batch_size=16):\n",
        "    # We'll take training samples in random order.\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,  # The training samples.\n",
        "        sampler=RandomSampler(train_dataset),  # Select batches randomly\n",
        "        batch_size=batch_size  # Trains with this batch size.\n",
        "    )\n",
        "    # For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "    validation_dataloader = DataLoader(\n",
        "        val_dataset,  # The validation samples.\n",
        "        sampler=SequentialSampler(val_dataset),  # Pull out batches sequentially.\n",
        "        batch_size=batch_size  # Evaluate with this batch size.\n",
        "    )\n",
        "\n",
        "    return [train_dataloader, validation_dataloader]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aXZLThlEc-O"
      },
      "outputs": [],
      "source": [
        "# convert labels to numerical values\n",
        "def convert_to_cat(labels, binary=False):\n",
        "    if not isinstance(labels, pd.Series):\n",
        "        labels = pd.Series(labels)\n",
        "    if binary:\n",
        "        main_val = labels.mode()[0]\n",
        "        other_val = [x for x in labels.unique() if x != main_val]\n",
        "        labels = np.where(labels != main_val, 1, 0)\n",
        "        corresp = pd.DataFrame([main_val, 'other'], columns=['old_label'])\n",
        "    elif np.issubdtype(labels.dtype, np.number) and (labels.min() >= 0):\n",
        "        print('labels already in a good format')\n",
        "        return {'labels': labels, 'categories': pd.DataFrame(labels.unique())}\n",
        "    else:\n",
        "        cats = labels.astype('category').cat\n",
        "        labels = cats.codes.astype('long')  # convert annotations to integers\n",
        "        corresp = pd.DataFrame(cats.categories, columns=['old_label'])\n",
        "    corresp.index.rename('new_label', inplace=True)\n",
        "    print('labels have been transformed for the model:\\n\\n', corresp)\n",
        "    return {'labels': labels, 'categories': corresp}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6TFG7b_Ec7P"
      },
      "outputs": [],
      "source": [
        "# Function to calculate performance of our predictions vs labels\n",
        "def perf_metrics(preds, labels, average='weighted', debug=False):\n",
        "    try:\n",
        "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "        # pred_flat = torch.max(pred_vec, 1)[1]\n",
        "    except:\n",
        "        print('only 1 dimension in labels prediction, no need for argmax')\n",
        "        pred_flat = preds.flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    acc = accuracy_score(labels_flat, pred_flat)\n",
        "    f1 = f1_score(labels_flat, pred_flat, average=average)\n",
        "    p = precision_score(labels_flat, pred_flat, average=average)\n",
        "    r = recall_score(labels_flat, pred_flat, average=average)\n",
        "    if debug:\n",
        "        print(\"PERF -- Acc: {:.3f} F1: {:.3f} Precision: {:.3f} Recall: {:.3f}\".format(acc, f1, p, r))\n",
        "    return {'f1': f1, 'acc': acc, 'p': p, 'r': r}\n",
        "\n",
        "\n",
        "def perf_metrics_classes(preds, labels):\n",
        "    try:\n",
        "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    except:\n",
        "        print('only 1 dimension in labels prediction, no need for argmax')\n",
        "        pred_flat = preds.flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    report = classification_report(labels_flat, pred_flat, output_dict=True)\n",
        "    df = pd.DataFrame(report).sort_index().transpose()\n",
        "    # df['accuracy'] = accuracy_score(labels, preds)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMi678-FEc4v"
      },
      "outputs": [],
      "source": [
        "# functions to handle different devices\n",
        "def to_cpu(vec, detach=True):\n",
        "    try:\n",
        "        vec = vec.detach().cpu().numpy() if detach else vec.to('cpu').numpy()\n",
        "    except:\n",
        "        vec = vec.detach().numpy() if detach else vec.numpy()\n",
        "    return vec\n",
        "\n",
        "\n",
        "def model_to_cpu(model):\n",
        "    try:\n",
        "        model.to(\"cpu\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "def batch_to_gpu(batch, device=None):\n",
        "    if device is None:\n",
        "        return batch\n",
        "    try:  # we're using a GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "    except:\n",
        "        batch = tuple(t for t in batch)\n",
        "    return batch\n",
        "\n",
        "def get_device():\n",
        "    # specify GPU device\n",
        "    try:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        if device != 'cpu':\n",
        "            print('number GPUs used:', torch.cuda.device_count())\n",
        "            print('device name:', torch.cuda.get_device_name(0))\n",
        "    except:\n",
        "        device = None\n",
        "        print('no CUDA capable device detected')\n",
        "    return device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yidGJpBEc1g"
      },
      "outputs": [],
      "source": [
        "def prep_BERT_dataset(sentences, labels=None, BERT_tokenizer='cambridgeltl/SapBERT-from-PubMedBERT-fulltext', MAX_TKN_LEN=511, debug=False):\n",
        "    \"\"\"\n",
        "    JC: change tokenizer to bert-base-uncased or 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext'\n",
        "\n",
        "    prepares data for BERT fine-tuning\n",
        "    :param sentences: list or array-like. list of texts to classify\n",
        "    :param labels: list or array-like, default None. list of labels corresponding to sentences\n",
        "    :param BERT_tokenizer: string, default 'bert-base-uncased'. BERT base model used\n",
        "    :param MAX_TKN_LEN: integer, default 511. see https://github.com/huggingface/transformers/issues/2446\n",
        "    :param debug: bool, default False. set to True to display inetrmediate results\n",
        "    :return: prepared dataset (torch Dataset) and nmber of dictinct labels\n",
        "    \"\"\"\n",
        "    sentences = pd.Series(sentences)\n",
        "    # load relevant data and add special tokens for BERT to work properly\n",
        "    sentences = [\"[CLS] \" + query + \" [SEP]\" for query in sentences]\n",
        "    if labels is not None:\n",
        "        labels = convert_to_cat(labels, binary=False)['labels']\n",
        "    else:\n",
        "        labels = pd.Series([1] * len(sentences))\n",
        "    if debug: print(sentences[0])\n",
        "\n",
        "    # Tokenize with BERT tokenizer\n",
        "    tokenizer = BertTokenizerFast.from_pretrained(BERT_tokenizer, do_lower_case=True)\n",
        "    if MAX_TKN_LEN is not None:\n",
        "        print('cutting the length of tokens to', MAX_TKN_LEN)\n",
        "        tokenized_texts = [tokenizer.tokenize(sent)[0:MAX_TKN_LEN] for sent in sentences]\n",
        "    else:\n",
        "        tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "    input_ids = [tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts]\n",
        "    if debug:\n",
        "        print(\"Tokenize the first sentence:\")\n",
        "        print(len(tokenized_texts), len(input_ids), len(input_ids[0]))\n",
        "        print(tokenized_texts[0], input_ids[0])\n",
        "\n",
        "    # add paddding to input_ids\n",
        "    input_ids_padded = pad_sequence([torch.tensor(i) for i in input_ids]).transpose(0, 1)\n",
        "    if debug: print(input_ids_padded.size(), len(input_ids_padded))\n",
        "\n",
        "    # Create attention masks\n",
        "    attention_masks = []\n",
        "    # Create a mask of 1s for each token followed by 0s for padding\n",
        "    for seq in input_ids_padded:\n",
        "        seq_mask = [float(i > 0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "\n",
        "    # create dataset\n",
        "    dataset = TensorDataset(input_ids_padded, torch.tensor(attention_masks), torch.tensor(labels))\n",
        "    num_labels = labels.nunique()\n",
        "    return {'dataset': dataset, 'num_labels': num_labels}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjJ5i1DfEcyo"
      },
      "outputs": [],
      "source": [
        "def finetune_BERT_pytorch(model, train_dataloader, validation_dataloader, n_epochs=3, output_dir='./sapbert_CE_may11/'):\n",
        "    \"\"\"\n",
        "    JC: update output_dir or say None\n",
        "\n",
        "    fine-tunes Bert for text classification\n",
        "    :param model: base Bert model\n",
        "    :param train_dataloader: Tensor Dataset. training data\n",
        "    :param validation_dataloader: Tensor Dataset. testing data\n",
        "    :param n_epochs: integer, default 5. number of epochs\n",
        "    :param output_dir: string, default None. directory where to save the fine-tuned model\n",
        "    :return:    model: fine-tuned Bert model with highest performance over k folds\n",
        "                stats: high level performance statistics\n",
        "                stats_classes: detailed performance statistics\n",
        "    \"\"\"\n",
        "    ###################################################################################\n",
        "    # BERT fine-tuning parameters\n",
        "    device = get_device()\n",
        "    try:\n",
        "        model.cuda()\n",
        "    except:\n",
        "        device = None\n",
        "        print('using CPU, this will be slow!')\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias', 'gamma', 'beta']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.0}\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, eps=1e-8)\n",
        "\n",
        "    # BERT training loop\n",
        "    train_loss_set = []\n",
        "    best_f1, best_epoch = 0, 0\n",
        "    for _ in trange(n_epochs, desc=\"Epoch\"):\n",
        "        ###################################################################################\n",
        "        ## TRAINING\n",
        "\n",
        "        # Set our model to training mode\n",
        "        model.train()\n",
        "        # Tracking variables\n",
        "        tr_loss, tr_perf, tr_perf_classes = 0, Counter({}), pd.DataFrame()\n",
        "        running_len = 0\n",
        "        # Train the data for one epoch\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            # Unpack the inputs from our dataloader (and move to GPU if using)\n",
        "            batch = batch_to_gpu(batch, device)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "            # Clear out the gradients (by default they accumulate)\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass\n",
        "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "            loss, logits = outputs['loss'], outputs['logits']\n",
        "            train_loss_set.append(loss.item())\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            # Update parameters and take a step using the computed gradient\n",
        "            optimizer.step()\n",
        "            # Update tracking variables\n",
        "            tr_loss += loss.item()\n",
        "            tmp_tr_perf = perf_metrics(to_cpu(logits), to_cpu(b_labels), average='weighted')\n",
        "            tmp_tr_perf.update((k, v * len(b_input_ids)) for k, v in tmp_tr_perf.items())\n",
        "            running_len += len(b_input_ids)\n",
        "            tr_perf = tr_perf + Counter(tmp_tr_perf)\n",
        "            tmp_tr_perf_classes = perf_metrics_classes(to_cpu(logits), to_cpu(b_labels))\n",
        "            tr_perf_classes = pd.concat((tr_perf_classes, tmp_tr_perf_classes))\n",
        "\n",
        "        # print('classes detail \\n\\n', tr_perf_classes)\n",
        "        tr_perf = {k: v / running_len for k, v in tr_perf.items()}\n",
        "        tr_perf_classes[['f1-score', 'precision', 'recall']] = tr_perf_classes[\n",
        "            ['f1-score', 'precision', 'recall']].multiply(tr_perf_classes['support'], axis=\"index\")\n",
        "        tr_perf_classes = tr_perf_classes.groupby(tr_perf_classes.index).sum()\n",
        "        tr_perf_classes[['f1-score', 'precision', 'recall']] = tr_perf_classes[['f1-score', 'precision', 'recall']].div(\n",
        "            tr_perf_classes['support'], axis=\"index\")\n",
        "        print('TRAIN - Loss: {:.3f} - F1: {:.3f} Acc: {:.3f} P: {:.3f} R: {:.3f}'.format(tr_loss / (1 + step),\n",
        "                                                                                         tr_perf['f1'], tr_perf['acc'],\n",
        "                                                                                         tr_perf['p'], tr_perf['r']))\n",
        "\n",
        "        ###################################################################################\n",
        "        ## VALIDATION\n",
        "\n",
        "        # Put model in evaluation mode\n",
        "        model.eval()\n",
        "        # Tracking variables\n",
        "        eval_perf, eval_perf_classes = Counter({}), pd.DataFrame()\n",
        "        running_len = 0\n",
        "        # Evaluate data for one epoch\n",
        "        for step, batch in enumerate(validation_dataloader):\n",
        "            # Unpack the inputs from our dataloader (and move to GPU if using)\n",
        "            batch = batch_to_gpu(batch, device)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "            with torch.no_grad():  # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "                # Forward pass, calculate logit predictions\n",
        "                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "                loss, logits = outputs['loss'], outputs['logits']\n",
        "            # Update tracking variables\n",
        "            tmp_eval_perf = perf_metrics(to_cpu(logits), to_cpu(b_labels), average='weighted')\n",
        "            tmp_eval_perf.update((k, v * len(b_input_ids)) for k, v in tmp_eval_perf.items())\n",
        "            # print('STEP:', step, 'LEN', len(b_input_ids), tmp_eval_perf)\n",
        "            running_len += len(b_input_ids)\n",
        "            eval_perf = eval_perf + Counter(tmp_eval_perf)\n",
        "            tmp_eval_perf_classes = perf_metrics_classes(to_cpu(logits), to_cpu(b_labels))\n",
        "            eval_perf_classes = pd.concat((eval_perf_classes, tmp_eval_perf_classes))\n",
        "\n",
        "        eval_perf = {k: v / running_len for k, v in\n",
        "                     eval_perf.items()}  # eval_perf = {k:v/(1+step) for k,v in eval_perf.items()}\n",
        "        eval_perf_classes[['f1-score', 'precision', 'recall']] = eval_perf_classes[\n",
        "            ['f1-score', 'precision', 'recall']].multiply(eval_perf_classes['support'], axis=\"index\")\n",
        "        eval_perf_classes = eval_perf_classes.groupby(eval_perf_classes.index).sum()\n",
        "        eval_perf_classes[['f1-score', 'precision', 'recall']] = eval_perf_classes[\n",
        "            ['f1-score', 'precision', 'recall']].div(eval_perf_classes['support'], axis=\"index\")\n",
        "        print('TEST -- F1: {:.3f} Acc: {:.3f} P: {:.3f} R: {:.3f}'.format(eval_perf['f1'], eval_perf['acc'],\n",
        "                                                                          eval_perf['p'], eval_perf['r']))\n",
        "\n",
        "        # store perf metrics and model\n",
        "        if eval_perf['f1'] >= best_f1:\n",
        "            best_f1 = eval_perf['f1']\n",
        "            best_epoch = _ + 1\n",
        "            stats_to_save = eval_perf\n",
        "            tr_perf_classes['dataset'] = 'train'\n",
        "            eval_perf_classes['dataset'] = 'test'\n",
        "            stats_classes_to_save = pd.concat([tr_perf_classes, eval_perf_classes])\n",
        "\n",
        "        if output_dir is not None: #save model with best F1\n",
        "            try:\n",
        "                print('saving...')\n",
        "                model.save_pretrained(output_dir)\n",
        "                stats_classes_to_save.to_csv(output_dir + '/stats.csv', header=True)\n",
        "            except:\n",
        "                print('model not saved, please enter valid path')\n",
        "        print('best F1 score obtained: {:.3f} at epoch {}'.format(best_f1, best_epoch))\n",
        "\n",
        "    return {'stats': stats_to_save, 'stats_classes': stats_classes_to_save, 'model': model}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JH2hf8gOEcwB"
      },
      "outputs": [],
      "source": [
        "def run_BERT(model, train_dataloader, validation_dataloader, n_epochs=3, output_dir='./sapbert_CE_may11/'):\n",
        "    \"\"\"\n",
        "    JC: update output_dir or say None\n",
        "\n",
        "    fine-tunes Bert for text classification\n",
        "    :param model: base Bert model\n",
        "    :param train_dataloader: Tensor Dataset. training data\n",
        "    :param validation_dataloader: Tensor Dataset. testing data\n",
        "    :param n_epochs: integer, default 5. number of epochs\n",
        "    :param output_dir: string, default None. directory where to save the fine-tuned model\n",
        "    :return:    model: fine-tuned Bert model with highest performance over k folds\n",
        "                stats: high level performance statistics\n",
        "                stats_classes: detailed performance statistics\n",
        "    \"\"\"\n",
        "    ###################################################################################\n",
        "    # BERT fine-tuning parameters\n",
        "    device = get_device()\n",
        "    try:\n",
        "        model.cuda()\n",
        "    except:\n",
        "        #device = None\n",
        "        print('could not move Bert to Cuda, using CPU, this will be slow!')\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias', 'gamma', 'beta']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.0}\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, eps=1e-8)\n",
        "    loss_fn = nn.CrossEntropyLoss() #JC try this?\n",
        "\n",
        "    # BERT training loop\n",
        "    train_loss_set = []\n",
        "    best_f1, best_epoch = 0, 0\n",
        "    for _ in trange(n_epochs, desc=\"Epoch\"):\n",
        "        ###################################################################################\n",
        "        ## TRAINING\n",
        "\n",
        "        # Set our model to training mode\n",
        "        model.train()\n",
        "        # Tracking variables\n",
        "        tr_loss, tr_perf, tr_perf_classes = 0, Counter({}), pd.DataFrame()\n",
        "        running_len = 0\n",
        "        # Train the data for one epoch\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            # Unpack the inputs from our dataloader (and move to GPU if using)\n",
        "            batch = batch_to_gpu(batch, device)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "            # Clear out the gradients (by default they accumulate)\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass\n",
        "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "            #loss, logits = outputs['loss'], outputs['logits']\n",
        "            #train_loss_set.append(loss.item())\n",
        "\n",
        "            #if using cross entropy then comment out loss, logits and use the lines below\n",
        "            ##loss = output[0] #keep this commented\n",
        "            logits = outputs[1]\n",
        "            loss = loss_fn(logits,b_labels)\n",
        "            print(loss)\n",
        "            train_loss_set.append(loss.item())\n",
        "\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            # Update parameters and take a step using the computed gradient\n",
        "            optimizer.step()\n",
        "            # Update tracking variables\n",
        "            tr_loss += loss.item()\n",
        "            tmp_tr_perf = perf_metrics(to_cpu(logits), to_cpu(b_labels), average='weighted')\n",
        "            tmp_tr_perf.update((k, v * len(b_input_ids)) for k, v in tmp_tr_perf.items())\n",
        "            running_len += len(b_input_ids)\n",
        "            tr_perf = tr_perf + Counter(tmp_tr_perf)\n",
        "            tmp_tr_perf_classes = perf_metrics_classes(to_cpu(logits), to_cpu(b_labels))\n",
        "            tr_perf_classes = pd.concat((tr_perf_classes, tmp_tr_perf_classes))\n",
        "\n",
        "        # print('classes detail \\n\\n', tr_perf_classes)\n",
        "        tr_perf = {k: v / running_len for k, v in tr_perf.items()}\n",
        "        tr_perf_classes[['f1-score', 'precision', 'recall']] = tr_perf_classes[\n",
        "            ['f1-score', 'precision', 'recall']].multiply(tr_perf_classes['support'], axis=\"index\")\n",
        "        tr_perf_classes = tr_perf_classes.groupby(tr_perf_classes.index).sum()\n",
        "        tr_perf_classes[['f1-score', 'precision', 'recall']] = tr_perf_classes[['f1-score', 'precision', 'recall']].div(\n",
        "            tr_perf_classes['support'], axis=\"index\")\n",
        "        print('TRAIN - Loss: {:.3f} - F1: {:.3f} Acc: {:.3f} P: {:.3f} R: {:.3f}'.format(tr_loss / (1 + step),\n",
        "                                                                                         tr_perf['f1'], tr_perf['acc'],\n",
        "                                                                                         tr_perf['p'], tr_perf['r']))\n",
        "\n",
        "        ###################################################################################\n",
        "        ## VALIDATION\n",
        "\n",
        "        # Put model in evaluation mode\n",
        "        model.eval()\n",
        "        # Tracking variables\n",
        "        eval_perf, eval_perf_classes = Counter({}), pd.DataFrame()\n",
        "        running_len = 0\n",
        "        # Evaluate data for one epoch\n",
        "        for step, batch in enumerate(validation_dataloader):\n",
        "            # Unpack the inputs from our dataloader (and move to GPU if using)\n",
        "            batch = batch_to_gpu(batch, device)\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "            with torch.no_grad():  # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "                # Forward pass, calculate logit predictions\n",
        "                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "                loss, logits = outputs['loss'], outputs['logits']\n",
        "            # Update tracking variables\n",
        "            tmp_eval_perf = perf_metrics(to_cpu(logits), to_cpu(b_labels), average='weighted')\n",
        "            tmp_eval_perf.update((k, v * len(b_input_ids)) for k, v in tmp_eval_perf.items())\n",
        "            # print('STEP:', step, 'LEN', len(b_input_ids), tmp_eval_perf)\n",
        "            running_len += len(b_input_ids)\n",
        "            eval_perf = eval_perf + Counter(tmp_eval_perf)\n",
        "            tmp_eval_perf_classes = perf_metrics_classes(to_cpu(logits), to_cpu(b_labels))\n",
        "            eval_perf_classes = pd.concat((eval_perf_classes, tmp_eval_perf_classes))\n",
        "\n",
        "        eval_perf = {k: v / running_len for k, v in\n",
        "                     eval_perf.items()}  # eval_perf = {k:v/(1+step) for k,v in eval_perf.items()}\n",
        "        eval_perf_classes[['f1-score', 'precision', 'recall']] = eval_perf_classes[\n",
        "            ['f1-score', 'precision', 'recall']].multiply(eval_perf_classes['support'], axis=\"index\")\n",
        "        eval_perf_classes = eval_perf_classes.groupby(eval_perf_classes.index).sum()\n",
        "        eval_perf_classes[['f1-score', 'precision', 'recall']] = eval_perf_classes[\n",
        "            ['f1-score', 'precision', 'recall']].div(eval_perf_classes['support'], axis=\"index\")\n",
        "        print('TEST -- F1: {:.3f} Acc: {:.3f} P: {:.3f} R: {:.3f}'.format(eval_perf['f1'], eval_perf['acc'],\n",
        "                                                                          eval_perf['p'], eval_perf['r']))\n",
        "\n",
        "        # store perf metrics and model\n",
        "        if eval_perf['f1'] >= best_f1:\n",
        "            best_f1 = eval_perf['f1']\n",
        "            best_epoch = _ + 1\n",
        "            stats_to_save = eval_perf\n",
        "            tr_perf_classes['dataset'] = 'train'\n",
        "            eval_perf_classes['dataset'] = 'test'\n",
        "            stats_classes_to_save = pd.concat([tr_perf_classes, eval_perf_classes])\n",
        "\n",
        "        if output_dir is not None: #save model with best F1\n",
        "            try:\n",
        "                print('saving...')\n",
        "                model.save_pretrained(output_dir)\n",
        "                stats_classes_to_save.to_csv(output_dir + '/stats.csv', header=True)\n",
        "            except:\n",
        "                print('model not saved, please enter valid path')\n",
        "        print('best F1 score obtained: {:.3f} at epoch {}'.format(best_f1, best_epoch))\n",
        "\n",
        "    return {'stats': stats_to_save, 'stats_classes': stats_classes_to_save, 'model': model}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGXDme2zEctA"
      },
      "outputs": [],
      "source": [
        "def train_BERT(sentences, labels, BERT_tokenizer='cambridgeltl/SapBERT-from-PubMedBERT-fulltext', test_size=0.1,\n",
        "                        n_epochs=3, batch_size=16, output_dir='./sapbert_CE_may11/', MAX_TKN_LEN=511):\n",
        "    \"\"\"\n",
        "    JC: update output_dir or say None;\n",
        "    JC: update tokenizer to bert-base-uncased or 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext'\n",
        "\n",
        "    formats dataset and fine-tunes Bert model on it\n",
        "    :param sentences: list or array-like. list of texts to classify\n",
        "    :param labels: list or array-like. list of labels corresponding to sentences\n",
        "    :param BERT_tokenizer: string, default 'bert-base-uncased'. BERT base model used\n",
        "    :param test_size: float, default 0.10. train/test split size\n",
        "    :param n_epochs: integer, default 5. number of epochs used to fine-tune Bert\n",
        "    :param batch_size: integer, default 32. how many samples pper batch to load\n",
        "    :param output_dir: string, default None. directory where to save the fine-tuned model\n",
        "    :param MAX_TKN_LEN: integer, default 511. see https://github.com/huggingface/transformers/issues/2446\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    print('formating dataset')\n",
        "    prep_data = prep_BERT_dataset(sentences=sentences, labels=labels, BERT_tokenizer=BERT_tokenizer,\n",
        "                                  MAX_TKN_LEN=MAX_TKN_LEN)\n",
        "    dataset = prep_data['dataset']\n",
        "    num_labels = prep_data['num_labels']\n",
        "    # split into train/test\n",
        "    print('splitting in train/test sets')\n",
        "    test_len = int(len(dataset) * test_size)\n",
        "    train_len = len(dataset) - test_len\n",
        "    print('test set:', test_len, 'train set:', train_len)\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_len, test_len])\n",
        "    # Create the DataLoaders for our training and validation sets.\n",
        "    train_dataloader, validation_dataloader = create_dataloader(train_dataset, val_dataset, batch_size=batch_size)\n",
        "    # Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top.\n",
        "    print('loading pre-trained BERT')\n",
        "    pretrained_model = BertForSequenceClassification.from_pretrained(BERT_tokenizer, num_labels=num_labels, output_attentions = False, output_hidden_states = False)\n",
        "    # train and evaluate BERT\n",
        "    print('training BERT')\n",
        "    res = finetune_BERT_pytorch(pretrained_model, train_dataloader, validation_dataloader, n_epochs=n_epochs, output_dir=output_dir)\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfN1PeAoEcnP"
      },
      "outputs": [],
      "source": [
        "# load pre-trained model and classify a new sentence\n",
        "def load_and_run_BERT(sentences, trained_bert_model, BERT_tokenizer='cambridgeltl/SapBERT-from-PubMedBERT-fulltext', MAX_TKN_LEN=511,\n",
        "                      batch_size=16):\n",
        "    \"\"\"\n",
        "    JC: update tokenizer to bert-base-uncased or 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext'\n",
        "\n",
        "    loads pre-trained Bert model and runs it on new text to classify\n",
        "    :param sentences: list or array-like. list of texts to classify\n",
        "    :param trained_bert_model: string or pytorch model. pre-trained model name or path\n",
        "    :param BERT_tokenizer: string, default 'bert-base-uncased'. BERT base model used\n",
        "    :param MAX_TKN_LEN: integer, default 511. see https://github.com/huggingface/transformers/issues/2446\n",
        "    :param batch_size: integer, default 32. how many samples pper batch to load\n",
        "    :return: dataframe of sentences classified along with probaility scores\n",
        "    \"\"\"\n",
        "    if isinstance(trained_bert_model, str):  # load trained BERT model if needed\n",
        "        trained_bert_model = BertForSequenceClassification.from_pretrained(trained_bert_model)\n",
        "    preds_class, probs, preds = [], [], pd.DataFrame()\n",
        "    sentences = pd.Series(sentences)\n",
        "    sentences_dataset = \\\n",
        "    prep_BERT_dataset(sentences, labels=None, BERT_tokenizer=BERT_tokenizer, MAX_TKN_LEN=MAX_TKN_LEN)['dataset']\n",
        "    # b_input_ids, b_input_mask, b_labels = sentences_dataset.tensors\n",
        "    validation_dataloader = DataLoader(sentences_dataset, sampler=SequentialSampler(sentences_dataset),\n",
        "                                       batch_size=batch_size)\n",
        "    for step, batch in enumerate(validation_dataloader):\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        with torch.no_grad():\n",
        "            outputs = trained_bert_model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "            loss, logits = outputs['loss'], outputs['logits']\n",
        "        preds_curr = logits.detach().numpy()\n",
        "        preds = preds.append(pd.DataFrame(preds_curr), ignore_index=True)\n",
        "        probs = np.append(probs, np.max(np.exp(preds_curr) / (1 + np.exp(preds_curr)), axis=1))\n",
        "        preds_class = np.append(preds_class, np.argmax(preds_curr, axis=1).flatten())\n",
        "\n",
        "    # put results in nice format\n",
        "    preds['sentences'] = sentences\n",
        "    preds['preds'] = preds_class\n",
        "    preds['probs'] = probs\n",
        "    #preds['label_goldstd'] = df['label'] #testing something - remove this if not needed or causing errors - use only when you want to do error analysis\n",
        "    return preds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdNEn7nbFbbo"
      },
      "outputs": [],
      "source": [
        "# TO RUN K-FOLD VALIDATION\n",
        "def run_KFOLD(dataset, base_model, model_trainer, base_model_loader=None,\n",
        "             n_splits=10, random_state=42, n_epochs=3, **kwargs):\n",
        "\n",
        "    kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
        "    labels = pd.Series([1] * len(dataset))\n",
        "    # tracking variables\n",
        "    best_f1, fold_nb = 0, 0\n",
        "    stats, stats_classes = pd.DataFrame(), pd.DataFrame()\n",
        "    # run k fold\n",
        "    for train_ix, test_ix in kfold.split(labels, labels):\n",
        "        fold_nb +=1\n",
        "        # need to load each time otherwise remembers training from previous fold\n",
        "        if base_model_loader is not None:\n",
        "            base_model_tmp = base_model_loader(base_model, **kwargs)\n",
        "        else:\n",
        "            base_model_tmp = base_model\n",
        "        print('####################### RUNNING FOLD:', fold_nb)\n",
        "        train_dataset = torch.utils.data.Subset(dataset, train_ix)\n",
        "        val_dataset = torch.utils.data.Subset(dataset, test_ix)\n",
        "        print(type(train_dataset), ' train set:', len(train_dataset), ' test set:',len(val_dataset))\n",
        "        train_dataloader, validation_dataloader = create_dataloader(train_dataset, val_dataset)\n",
        "        print('training and evaluating model')\n",
        "        res = model_trainer(base_model_tmp, train_dataloader, validation_dataloader, n_epochs=n_epochs, output_dir=None)\n",
        "        del base_model_tmp\n",
        "        # store perf metrics and model\n",
        "        stats_tmp = pd.DataFrame.from_dict(res['stats'],orient='index', columns=['value'])\n",
        "        stats_tmp['fold'] = fold_nb\n",
        "        stats = pd.concat([stats, stats_tmp])\n",
        "        res['stats_classes']['fold'] = fold_nb\n",
        "        stats_classes = pd.concat([stats_classes, res['stats_classes']])\n",
        "        if res['stats']['f1'] >= best_f1:\n",
        "            best_f1 = res['stats']['f1']\n",
        "            res_to_save = res\n",
        "\n",
        "\n",
        "    print('best F1 score obtained across splits: {:.3f}'.format(best_f1))\n",
        "    return {'stats':stats, 'stats_classes':stats_classes, 'model':res_to_save['model']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_PPYfy7FbYa"
      },
      "outputs": [],
      "source": [
        "def BERT_KFOLD(sentences, labels, BERT_tokenizer='cambridgeltl/SapBERT-from-PubMedBERT-fulltext',\n",
        "               n_splits=10, random_state=42, n_epochs=3, output_dir='./sapbert_CE_may11/', MAX_TKN_LEN=511):\n",
        "    \"\"\"\n",
        "    JC: update output_dir or say None;\n",
        "    JC: update tokenizer to bert-base-uncased or 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext'\n",
        "\n",
        "    fine-tunes Bert model using k-old cross validation\n",
        "    :param sentences: list or array-like. list of texts to classify\n",
        "    :param labels: list or array-like. list of labels corresponding to sentences\n",
        "    :param BERT_tokenizer: string, default 'bert-base-uncased'. BERT base model used\n",
        "    :param n_splits: integer, default None. number of folds to use\n",
        "    :param random_state: integer, default 42. random seed to initialize folds generation\n",
        "    :param n_epochs: integer, default 5. number of epochs used to fine-tune Bert\n",
        "    :param output_dir: string, default None. directory where to save the fine-tuned model\n",
        "    :param MAX_TKN_LEN: integer, default 511. see https://github.com/huggingface/transformers/issues/2446\n",
        "    :return:    model: fine-tuned Bert model with highest performance over k folds\n",
        "                stats: high level performance statistics\n",
        "                stats_classes: detailed performance statistics\n",
        "    \"\"\"\n",
        "    prep_data = prep_BERT_dataset(sentences=sentences, labels=labels, BERT_tokenizer=BERT_tokenizer,\n",
        "                                  MAX_TKN_LEN=MAX_TKN_LEN)\n",
        "    dataset = prep_data['dataset']\n",
        "    res = run_KFOLD(dataset=dataset, base_model=BERT_tokenizer, model_trainer=run_BERT,\n",
        "                    base_model_loader=BertForSequenceClassification.from_pretrained,\n",
        "                    n_splits=n_splits, random_state=random_state, n_epochs=n_epochs, num_labels=prep_data['num_labels'])\n",
        "    if output_dir is not None:\n",
        "        try:\n",
        "            print('saving model in:', output_dir)\n",
        "            res['model'].save_pretrained(output_dir)\n",
        "            res['stats_classes'].to_csv(output_dir + '/stats.csv', header=True)\n",
        "        except:\n",
        "            print('model not saved, please enter valid path')\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86Jg7VkJd1vp"
      },
      "outputs": [],
      "source": [
        "#Confidence Intervals (Credit to Diana Shamsutdinova)\n",
        "\n",
        "def f1_conf_int(target, predict, bootstrap_n = 500):\n",
        "    n = len(target)\n",
        "    f1 = np.zeros(bootstrap_n)\n",
        "    for bs in np.arange(bootstrap_n):\n",
        "        i = np.random.choice(n, n, replace = True)\n",
        "        tp = pd.DataFrame({\"t\": target, \"p\": predict})\n",
        "        tp_bootstrap = tp.iloc[i, :]\n",
        "        f1[bs] = (f1_score(tp_bootstrap[\"t\"], tp_bootstrap[\"p\"], average='weighted')) #added , average='weighted'\n",
        "    res = [f1.mean(), np.quantile(f1,0.025), np.quantile(f1,0.975), f1.std()]\n",
        "    return res\n",
        "\n",
        "def precision_conf_int(target, predict, bootstrap_n = 500):\n",
        "    n = len(target)\n",
        "    p = np.zeros(bootstrap_n)\n",
        "    for bs in np.arange(bootstrap_n):\n",
        "        i = np.random.choice(n, n, replace = True)\n",
        "        tp = pd.DataFrame({\"t\": target, \"p\": predict})\n",
        "        tp_bootstrap = tp.iloc[i, :]\n",
        "        p[bs] = (precision_score(tp_bootstrap[\"t\"], tp_bootstrap[\"p\"], average='weighted')) #added , average='weighted'\n",
        "    res = [p.mean(), np.quantile(p,0.025), np.quantile(p,0.975), p.std()]\n",
        "    return res\n",
        "\n",
        "def recall_conf_int(target, predict, bootstrap_n = 500):\n",
        "    n = len(target)\n",
        "    r = np.zeros(bootstrap_n)\n",
        "    for bs in np.arange(bootstrap_n):\n",
        "        i = np.random.choice(n, n, replace = True)\n",
        "        tp = pd.DataFrame({\"t\": target, \"p\": predict})\n",
        "        tp_bootstrap = tp.iloc[i, :]\n",
        "        r[bs] = (recall_score(tp_bootstrap[\"t\"], tp_bootstrap[\"p\"], average='weighted')) #added , average='weighted'\n",
        "    res = [r.mean(), np.quantile(r,0.025), np.quantile(r,0.975), r.std()]\n",
        "    return res\n",
        "\n",
        "def class_0_f1_conf_int(target, predict, bootstrap_n = 500):\n",
        "    n = len(target)\n",
        "    r = np.zeros(bootstrap_n)\n",
        "    for bs in np.arange(bootstrap_n):\n",
        "        i = np.random.choice(n, n, replace = True)\n",
        "        tp = pd.DataFrame({\"t\": target, \"p\": predict})\n",
        "        tp_bootstrap = tp.iloc[i, :]\n",
        "        r[bs] = (f1_score(tp_bootstrap[\"t\"], tp_bootstrap[\"p\"], pos_label=0, average='binary'))\n",
        "    res = [r.mean(), np.quantile(r,0.025), np.quantile(r,0.975), r.std()]\n",
        "    return res\n",
        "\n",
        "def class_1_f1_conf_int(target, predict, bootstrap_n = 500):\n",
        "    n = len(target)\n",
        "    r = np.zeros(bootstrap_n)\n",
        "    for bs in np.arange(bootstrap_n):\n",
        "        i = np.random.choice(n, n, replace = True)\n",
        "        tp = pd.DataFrame({\"t\": target, \"p\": predict})\n",
        "        tp_bootstrap = tp.iloc[i, :]\n",
        "        r[bs] = (f1_score(tp_bootstrap[\"t\"], tp_bootstrap[\"p\"], pos_label=1, average='binary'))\n",
        "    res = [r.mean(), np.quantile(r,0.025), np.quantile(r,0.975), r.std()]\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mw0BPkukFbWA"
      },
      "outputs": [],
      "source": [
        "# fine tune Bert - update output_dir or say None\n",
        "bert_model = train_BERT(sentences=data_for_classifier['context'].to_list(), labels=data_for_classifier['label'].to_list(), test_size=0.2, n_epochs=5, output_dir='./sapbert_CE_may11/')\n",
        "bert_model['stats']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyoNgVyId1vq"
      },
      "outputs": [],
      "source": [
        "bert_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWpHzLCSFbTJ"
      },
      "outputs": [],
      "source": [
        "# run pre-trained model --- doesn't work - RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking arugment for argument index in method wrapper_index_select)\n",
        "load_and_run_BERT(sentences=['hello my name is link i am in love with princess zelda', 'this is just a test sentence'],\n",
        "                  trained_bert_model=bert_model['model'],\n",
        "                  BERT_tokenizer=BERT_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bqCiEEYIIRd"
      },
      "outputs": [],
      "source": [
        "# run k-fold validation\n",
        "kf = BERT_KFOLD(sentences=data_for_classifier['context'].to_list(), labels=data_for_classifier['label'].to_list(), n_splits=10, BERT_tokenizer=BERT_tokenizer, n_epochs=5,\n",
        "                random_state=666)\n",
        "print(kf['stats'], '\\n\\n', kf['stats_classes'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBLkcK-XIIIW"
      },
      "outputs": [],
      "source": [
        "kf['stats_classes'].to_csv('./sapbert_CE_may11/kf_stats_classes.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff9J5rjEIH_w"
      },
      "outputs": [],
      "source": [
        "kf['stats'].to_csv('./sapbert_CE_may11/kf_stats.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gTpy7_sd1vq"
      },
      "source": [
        "confidence intervals - this wont work here because the labels and pred variables are from the second bert method below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "141H7QzOd1vq"
      },
      "outputs": [],
      "source": [
        "f1_conf_int(flat_true_labels, flat_predictions) #gives mean, lower ci, upper ci, std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6F04460d1vr"
      },
      "outputs": [],
      "source": [
        "f1_score(flat_true_labels, flat_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laus95aVd1vr"
      },
      "outputs": [],
      "source": [
        "class_0_f1_conf_int(flat_true_labels, flat_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6Eex1HTd1vr"
      },
      "outputs": [],
      "source": [
        "class_1_f1_conf_int(flat_true_labels, flat_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8tbmMEZd1vr"
      },
      "outputs": [],
      "source": [
        "print(classification_report(flat_true_labels, flat_predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wEHwdRSd1vr"
      },
      "outputs": [],
      "source": [
        "bert_report = classification_report(flat_true_labels, flat_predictions, output_dict=True)\n",
        "df_bert = pd.DataFrame(bert_report).transpose()\n",
        "df_bert.to_csv('./sapbert_CE_may11/classification_report.csv', index= True)\n",
        "df_bert.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5yZWs1Td1vr"
      },
      "outputs": [],
      "source": [
        "data_for_classifier.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QKph3BRd1vr"
      },
      "outputs": [],
      "source": [
        "testing_data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cxKTjt4d1vs"
      },
      "outputs": [],
      "source": [
        "testing_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIObldsWEcg_"
      },
      "outputs": [],
      "source": [
        "# RUN PRETRAINED BERT AND ANALYSE ERRORS\n",
        "BERT_tokenizer = 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext' #or can be 'bert-base-uncased'\n",
        "# model_path = data_path+'bert_models/'+dom+'_biobert'\n",
        "model_path = './sapbert_CE_may11' #update accordingly\n",
        "#text_col = 'clean_text'\n",
        "text_col = 'context'\n",
        "\n",
        "#df = pd.read_excel(data_path+'/cognition_adjudicated_clean.xlsx')\n",
        "df = pd.read_csv('test_data_pain.csv', usecols=['label', 'context', 'BrcID', 'docid', 'text', 'value'])\n",
        "                 #columns=['Brcid', 'CN_Doc_ID','doc_date', 'doc_type', 'kw', 'domain', 'start_kw', 'clean_text'])\n",
        "# df = df.loc[df.domain == dom].reset_index()\n",
        "df[text_col] = df[text_col].apply(lambda x: str(x).strip().lower())\n",
        "\n",
        "res= load_and_run_BERT(sentences=df[text_col]\n",
        "                       , trained_bert_model=model_path\n",
        "                       , BERT_tokenizer=BERT_tokenizer)\n",
        "\n",
        "\n",
        "res.to_csv(model_path + './sapbert_CE_may11/sapbert_CE_may11_testdata.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSG2dDB6d1vs"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k87s6_P8d1vs"
      },
      "outputs": [],
      "source": [
        "df.to_csv('test_with_clean_text.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "HpAerbLOd1vs"
      },
      "source": [
        "# to run on cohort pain sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-Ujyzh6d1vt"
      },
      "outputs": [],
      "source": [
        "#df = pd.read_csv('./classified_as_pain_sapbert_CE.csv')\n",
        "df = pd.read_csv('./test_data_pain.csv')\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FahaXAFDd1vt"
      },
      "outputs": [],
      "source": [
        "df = df.drop(columns='Unnamed: 0')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hg4AkjVbd1vt"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlQ3HzK_d1vt"
      },
      "outputs": [],
      "source": [
        "df2 = df[:2000]\n",
        "#df2 = df[2001:5000]\n",
        "#df2 = df[5001:8000]\n",
        "#df2 = df[8001:10000]\n",
        "#df2 = df[10001:20000]\n",
        "#df2 = df[20001:30000]\n",
        "#df2 = df[30001:40000]\n",
        "#df2 = df[40001:50000]\n",
        "#df2 = df[50001:60000]\n",
        "#df2 = df[60001:70000]\n",
        "#df2 = df[70001:80000]\n",
        "#df2 = df[80001:90000]\n",
        "#df2 = df[90001:98940] #for anatomy\n",
        "#df2 = df[90001:100000]\n",
        "#df2 = df[100001:110000]\n",
        "#df2 = df[110001:120000]\n",
        "#df2 = df[120001:130000]\n",
        "#df2 = df[130001:140000]\n",
        "#df2 = df[140001:150000]\n",
        "#df2 = df[150001:160000]\n",
        "#df2 = df[160001:174042]\n",
        "df2.to_csv('./anatomy_pain_sentences_for_classifier_90k_to_99k.csv')\n",
        "\n",
        "df2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zf9UeEGQd1vt"
      },
      "outputs": [],
      "source": [
        "df2.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "jHNDp6L4d1vu"
      },
      "outputs": [],
      "source": [
        "# RUN BERT ON LARGE FILE (EXEC LINE BY LINE) -- use this when running on new CRIS extraction -- update file names!!!\n",
        "cpt=0\n",
        "BERT_tokenizer = 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext' #'bert-base-uncased'\n",
        "#BERT_tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased') #'bert_base_uncased' #this seems to give a url error? and link that asks to sign in to huggingface\n",
        "#model_path = data_path+'/bert_models/cognition_all_biobert_rel10'\n",
        "model_path = './sapbert_model_dec13_second_script_crossentropy' #'./bert_base_model_nov14_second_script_notoverfit'\n",
        "#names=['BrcId', 'CN_Doc_ID','doc_date', 'doc_type', 'kw', 'domain', 'start_kw', 'clean_text']#, 'age', 'ethnicitycleaned', 'Gender_ID']\n",
        "names=['context', 'label']\n",
        "for chunk in pd.read_csv('./test_data_pain.csv'\n",
        "                         , chunksize=10000): #, names=names):\n",
        "    cpt+=1\n",
        "    print('chunk nb',cpt)\n",
        "    res= load_and_run_BERT(sentences=chunk['context'] #['clean_text']\n",
        "                   , trained_bert_model=model_path\n",
        "                   , BERT_tokenizer=BERT_tokenizer)\n",
        "\n",
        "    result = pd.concat([res.reset_index()[['preds','probs']]\n",
        "                        , chunk.reset_index()[['context', 'label']]] #'BrcId', 'CN_Doc_ID','doc_date', 'doc_type', 'kw', 'domain', 'start_kw', 'clean_text']]]\n",
        "                       , axis=1)\n",
        "    print(result.head(1))\n",
        "    header_bool=1 if cpt==1 else False\n",
        "    result[['preds','probs']+names].to_csv('./test_data_pain_sapbert_CE_may11.csv', mode='a', header=header_bool)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkNIUAk8d1vu"
      },
      "outputs": [],
      "source": [
        "# this is taking about 1hr40 mins? for 1000 sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "eZkqxsrkd1vv"
      },
      "source": [
        "# BERT Classification 2\n",
        "\n",
        "This will let you get confidence intervals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlhifD6Bd1vv"
      },
      "source": [
        "Another BERT training script - gives more details on loss, tokenisation, matthew's corr etc - and confidence intervals for f1, p and r"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training data"
      ],
      "metadata": {
        "id": "65UBsa5HHetp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myEhqm9txHCm"
      },
      "outputs": [],
      "source": [
        "training_data.loc[training_data.label == 0].sample(5)[['context', 'label']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YxhosJzxOzt"
      },
      "outputs": [],
      "source": [
        "training_data.loc[training_data.label == 1].sample(5)[['context', 'label']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHcBljs3xTjH"
      },
      "outputs": [],
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "sentences = training_data.context.values\n",
        "labels = training_data.label.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOK6CLDsxan9"
      },
      "outputs": [],
      "source": [
        "type(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "YmV35ygMHgOi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQ8kzBOmxhMz"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, AutoTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\", do_lower_case=True)\n",
        "\n",
        "#comparing tokenizers\n",
        "bert_base_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "sapbert_tokenizer = AutoTokenizer.from_pretrained(\"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\", do_lower_case=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenise"
      ],
      "metadata": {
        "id": "b5nTqwMIHj_c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfvoF5kKd1vw"
      },
      "outputs": [],
      "source": [
        "#testing some sentences out\n",
        "sentences = ['e control of her anxiety much more effectively. we discussed regular practice of mindful breathing and attention control aimed at combating the catastrophic thinking and worry about symptoms of vulvodynia. we discussed pelvic floor relaxation exercises and importance of keeping up the practice. next session 04/01/18 10am. ---------------------------11 dec 2017 11:51 sylvia hejda-forde']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nD0RaHJOxlcI"
      },
      "outputs": [],
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "#print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "#print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Bert_base Tokenized: ', bert_base_tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('SAPBERT Tokenized: ', sapbert_tokenizer.tokenize(sentences[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4_hb0FuxsVC"
      },
      "outputs": [],
      "source": [
        "max_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qc2am7q5x0QC"
      },
      "outputs": [],
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 64,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "\n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split data"
      ],
      "metadata": {
        "id": "454g3y9eHrkk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtMmRu4CyFsL"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Create a 90-10 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load data"
      ],
      "metadata": {
        "id": "9bW6-9ZDHxsP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwTafW97yOG-"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it\n",
        "# here. For fine-tuning BERT on a specific task, the authors recommend a batch\n",
        "# size of 16 or 32.\n",
        "batch_size = 16 #tried 32 but 16 worked better for train and val loss\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order.\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load pretrained model"
      ],
      "metadata": {
        "id": "CX74W7TzHzLS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1mpf2dOyN8d"
      },
      "outputs": [],
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig, AutoModelForSequenceClassification\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single\n",
        "# linear classification layer on top.\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "#model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    #\"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    \"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\",\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.\n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print parameters"
      ],
      "metadata": {
        "id": "LBksy6iKH2Y0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTk50as5yNwC"
      },
      "outputs": [],
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b42b77O_yg5i"
      },
      "outputs": [],
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch)\n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 3e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5 - changed back to 5e-5 to reduce overfitting - didnt work so tried 3e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        ")\n",
        "#loss_fn = nn.CrossEntropyLoss() #try this"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up hyperparameters"
      ],
      "metadata": {
        "id": "swWuAUu6H5MM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFT2UcPJyg19"
      },
      "outputs": [],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4.\n",
        "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "# training data.\n",
        "epochs = 3 #changed from 4 because it looked like it was overfitting at 4 and 8 and 2 - changed back to 4 to test with diff learning rate\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs].\n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDR9lS8fytZj"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLF-romzoxvz"
      },
      "outputs": [],
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and validate"
      ],
      "metadata": {
        "id": "9eMHaGflH9Tx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dl-mFq6OytW9"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss,\n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "\n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to\n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "\n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader.\n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the\n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because\n",
        "        # accumulating the gradients is \"convenient while training RNNs\".\n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # In PyTorch, calling `model` will in turn call the model's `forward`\n",
        "        # function and pass down the arguments. The `forward` function is\n",
        "        # documented here:\n",
        "        # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n",
        "        # The results are returned in a results object, documented here:\n",
        "        # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n",
        "        # Specifically, we'll get the loss (because we provided labels) and the\n",
        "        # \"logits\"--the model outputs prior to activation.\n",
        "        result = model(b_input_ids,\n",
        "                       token_type_ids=None,\n",
        "                       attention_mask=b_input_mask,\n",
        "                       labels=b_labels,\n",
        "                       return_dict=True)\n",
        "\n",
        "        loss = result.loss\n",
        "        logits = result.logits\n",
        "        #loss = loss_fn(logits,b_labels) #try this instead of loss=result.loss ?\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value\n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "\n",
        "        # Unpack this training batch from our dataloader.\n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using\n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which\n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            result = model(b_input_ids,\n",
        "                           token_type_ids=None,\n",
        "                           attention_mask=b_input_mask,\n",
        "                           labels=b_labels,\n",
        "                           return_dict=True)\n",
        "\n",
        "        # Get the loss and \"logits\" output by the model. The \"logits\" are the\n",
        "        # output values prior to applying an activation function like the\n",
        "        # softmax.\n",
        "        loss = result.loss\n",
        "        logits = result.logits\n",
        "\n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OWUfx-cytUo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Display floats with two decimal places.\n",
        "pd.set_option('precision', 2)\n",
        "\n",
        "# Create a DataFrame from our training statistics.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Use the 'epoch' as the row index.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# A hack to force the column headers to wrap.\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Display the table.\n",
        "df_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7FvDPXXzs0D"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3])\n",
        "\n",
        "plt.show()\n",
        "\n",
        "plt.savefig('training_validation_loss.png')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdFRCAaJytSC"
      },
      "outputs": [],
      "source": [
        "#performance on test set\n",
        "\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(testing_data.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = testing_data.context.values\n",
        "labels = testing_data.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode_plus` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    #   (5) Pad or truncate the sentence to `max_length`\n",
        "    #   (6) Create attention masks for [PAD] tokens.\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 64,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "\n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48d5z46pytQo"
      },
      "outputs": [],
      "source": [
        "#Evaluate on test set\n",
        "\n",
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict\n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "  # Telling the model not to compute or store gradients, saving memory and\n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions.\n",
        "      result = model(b_input_ids,\n",
        "                     token_type_ids=None,\n",
        "                     attention_mask=b_input_mask,\n",
        "                     return_dict=True)\n",
        "\n",
        "  logits = result.logits\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnPr91aboPc5"
      },
      "outputs": [],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NshCX410oR-3"
      },
      "outputs": [],
      "source": [
        "true_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQaNv54eytMj"
      },
      "outputs": [],
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (testing_data.label.sum(), len(testing_data.label), (testing_data.label.sum() / len(testing_data.label) * 100.0)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWSwTp56d1v3"
      },
      "source": [
        "Confidence Interval for BERT model! - run this after the MCC stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFRBwhMad1v3"
      },
      "outputs": [],
      "source": [
        "###insert functions for f1_conf_int, precision and recall and per label then run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egSQV8UQd1v3"
      },
      "outputs": [],
      "source": [
        "f1_conf_int(flat_true_labels, flat_predictions) #gives f1 mean, ci lower, ci upper, std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64PXdBNrd1v3"
      },
      "outputs": [],
      "source": [
        "class_0_f1_conf_int(flat_true_labels, flat_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIKWd_TAd1v3"
      },
      "outputs": [],
      "source": [
        "class_1_f1_conf_int(flat_true_labels, flat_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vq-ncs1ad1v4"
      },
      "outputs": [],
      "source": [
        "f1_score(flat_true_labels, flat_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Dz53_STd1v4"
      },
      "outputs": [],
      "source": [
        "print(classification_report(flat_true_labels, flat_predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kLRGd6Ad1v4"
      },
      "outputs": [],
      "source": [
        "precision_conf_int(flat_true_labels, flat_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gPHjCPCd1v4"
      },
      "outputs": [],
      "source": [
        "recall_conf_int(flat_true_labels, flat_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNViK7C3d1v4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BqDbsztytHk"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "\n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\"\n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "\n",
        "  # Calculate and store the coef for this batch.\n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)\n",
        "  matthews_set.append(matthews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_nu4G6zygmR"
      },
      "outputs": [],
      "source": [
        "# Create a barplot showing the MCC score for each batch of test samples.\n",
        "ax = sns.barplot(x=list(range(len(matthews_set))), y=matthews_set, ci=None)\n",
        "\n",
        "plt.title('MCC Score per Batch')\n",
        "plt.ylabel('MCC Score (-1 to +1)')\n",
        "plt.xlabel('Batch #')\n",
        "\n",
        "plt.savefig('MCC_bert_base.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSHRU-Emyga1"
      },
      "outputs": [],
      "source": [
        "# Combine the results across all batches.\n",
        "flat_predictions = np.concatenate(predictions, axis=0)\n",
        "\n",
        "# For each sample, pick the label (0 or 1) with the higher score.\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('Total MCC: %.3f' % mcc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJqravM305y0"
      },
      "outputs": [],
      "source": [
        "#saving model - not done\n",
        "\n",
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = './bert_base_model_nov14_second_script_notoverfit'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "#torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uppDzBKd05wL"
      },
      "outputs": [],
      "source": [
        "#file sizes\n",
        "\n",
        "!ls -l --block-size=K './'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIh_MLNb05tE"
      },
      "outputs": [],
      "source": [
        "!ls -l --block-size=M './pytorch_model.bin'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8sqUUer-nCV"
      },
      "outputs": [],
      "source": [
        "model = torch.load('./pytorch_model.bin')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlpfE66cUhPp"
      },
      "source": [
        "Need F1 score as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYwNNd7AioFt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import TensorDataset\n",
        "from tqdm import trange\n",
        "from transformers import BertForSequenceClassification, BertTokenizerFast, AdamW, AutoModel, AutoModelForSequenceClassification, AutoTokenizer  #BertTokenizer #AutoModel was missing, added in - JC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfHXqS279Qf0"
      },
      "outputs": [],
      "source": [
        "# run k-fold validation\n",
        "kf = BERT_KFOLD(sentences=data_for_classifier['text'].to_list(), labels=data_for_classifier['label'].to_list(), n_splits=5, BERT_tokenizer=BERT_tokenizer, n_epochs=1,\n",
        "                random_state=666)\n",
        "print(kf['stats'], '\\n\\n', kf['stats_classes'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEt9zdGK9_2g"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "cW5XCACSd1v6"
      },
      "source": [
        "# GPT-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55VLyUPud1v6"
      },
      "source": [
        "https://colab.research.google.com/github/gmihaila/ml_things/blob/master/notebooks/pytorch/gpt2_finetune_classification.ipynb#scrollTo=EDEubgJIt23C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUPQF2vGd1v7"
      },
      "outputs": [],
      "source": [
        "# Install helper functions.\n",
        "!pip install -q git+https://github.com/gmihaila/ml_things.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RT9YJlQAd1v7"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import os\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from ml_things import plot_dict, plot_confusion_matrix, fix_text\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from transformers import (set_seed,\n",
        "                          TrainingArguments,\n",
        "                          Trainer,\n",
        "                          GPT2Config,\n",
        "                          GPT2Tokenizer,\n",
        "                          AdamW,\n",
        "                          get_linear_schedule_with_warmup,\n",
        "                          GPT2ForSequenceClassification)\n",
        "\n",
        "# Set seed for reproducibility.\n",
        "set_seed(123)\n",
        "\n",
        "# Number of training epochs (authors on fine-tuning Bert recommend between 2 and 4).\n",
        "epochs = 4\n",
        "\n",
        "# Number of batches - depending on the max sequence length and GPU memory.\n",
        "# For 512 sequence length batch of 10 works without cuda memory issues.\n",
        "# For small sequence length can try batch of 32 or higher.\n",
        "batch_size = 32\n",
        "\n",
        "# Pad or truncate text sequences to a specific length\n",
        "# if `None` it will use maximum sequence of word piece tokens allowed by model.\n",
        "max_length = 60\n",
        "\n",
        "# Look for gpu to use. Will use `cpu` by default if no gpu found.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Name of transformers model - will use already pretrained model.\n",
        "# Path of transformer model - will load your own model from local disk.\n",
        "model_name_or_path = 'gpt2'\n",
        "\n",
        "# Dictionary of labels and their id - this will be used to convert.\n",
        "# String labels to number ids.\n",
        "labels_ids = {'not relevant': 0, 'relevant': 1}\n",
        "\n",
        "# How many labels are we using in training.\n",
        "# This is used to decide size of classification head.\n",
        "n_labels = len(labels_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rfn5ocnLd1v7"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Gpt2ClassificationCollator(object):\n",
        "    r\"\"\"\n",
        "    Data Collator used for GPT2 in a classificaiton rask.\n",
        "\n",
        "    It uses a given tokenizer and label encoder to convert any text and labels to numbers that\n",
        "    can go straight into a GPT2 model.\n",
        "\n",
        "    This class is built with reusability in mind: it can be used as is as long\n",
        "    as the `dataloader` outputs a batch in dictionary format that can be passed\n",
        "    straight into the model - `model(**batch)`.\n",
        "\n",
        "    Arguments:\n",
        "\n",
        "      use_tokenizer (:obj:`transformers.tokenization_?`):\n",
        "          Transformer type tokenizer used to process raw text into numbers.\n",
        "\n",
        "      labels_ids (:obj:`dict`):\n",
        "          Dictionary to encode any labels names into numbers. Keys map to\n",
        "          labels names and Values map to number associated to those labels.\n",
        "\n",
        "      max_sequence_len (:obj:`int`, `optional`)\n",
        "          Value to indicate the maximum desired sequence to truncate or pad text\n",
        "          sequences. If no value is passed it will used maximum sequence size\n",
        "          supported by the tokenizer and model.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, use_tokenizer, labels_encoder, max_sequence_len=None):\n",
        "\n",
        "        # Tokenizer to be used inside the class.\n",
        "        self.use_tokenizer = use_tokenizer\n",
        "        # Check max sequence length.\n",
        "        self.max_sequence_len = use_tokenizer.model_max_length if max_sequence_len is None else max_sequence_len\n",
        "        # Label encoder used inside the class.\n",
        "        self.labels_encoder = labels_encoder\n",
        "\n",
        "        return\n",
        "\n",
        "    def __call__(self, sequences):\n",
        "        r\"\"\"\n",
        "        This function allowes the class objesct to be used as a function call.\n",
        "        Sine the PyTorch DataLoader needs a collator function, I can use this\n",
        "        class as a function.\n",
        "\n",
        "        Arguments:\n",
        "\n",
        "          item (:obj:`list`):\n",
        "              List of texts and labels.\n",
        "\n",
        "        Returns:\n",
        "          :obj:`Dict[str, object]`: Dictionary of inputs that feed into the model.\n",
        "          It holddes the statement `model(**Returned Dictionary)`.\n",
        "        \"\"\"\n",
        "\n",
        "        # Get all texts from sequences list.\n",
        "        texts = [sequence['text'] for sequence in sequences]\n",
        "        # Get all labels from sequences list.\n",
        "        labels = [sequence['label'] for sequence in sequences]\n",
        "        # Encode all labels using label encoder.\n",
        "        labels = [self.labels_encoder[label] for label in labels]\n",
        "        # Call tokenizer on all texts to convert into tensors of numbers with\n",
        "        # appropriate padding.\n",
        "        inputs = self.use_tokenizer(text=texts, return_tensors=\"pt\", padding=True, truncation=True,  max_length=self.max_sequence_len)\n",
        "        # Update the inputs with the associated encoded labels as tensor.\n",
        "        inputs.update({'labels':torch.tensor(labels)})\n",
        "\n",
        "        return inputs\n",
        "\n",
        "\n",
        "def train(dataloader, optimizer_, scheduler_, device_):\n",
        "  r\"\"\"\n",
        "  Train pytorch model on a single pass through the data loader.\n",
        "\n",
        "  It will use the global variable `model` which is the transformer model\n",
        "  loaded on `_device` that we want to train on.\n",
        "\n",
        "  This function is built with reusability in mind: it can be used as is as long\n",
        "    as the `dataloader` outputs a batch in dictionary format that can be passed\n",
        "    straight into the model - `model(**batch)`.\n",
        "\n",
        "  Arguments:\n",
        "\n",
        "      dataloader (:obj:`torch.utils.data.dataloader.DataLoader`):\n",
        "          Parsed data into batches of tensors.\n",
        "\n",
        "      optimizer_ (:obj:`transformers.optimization.AdamW`):\n",
        "          Optimizer used for training.\n",
        "\n",
        "      scheduler_ (:obj:`torch.optim.lr_scheduler.LambdaLR`):\n",
        "          PyTorch scheduler.\n",
        "\n",
        "      device_ (:obj:`torch.device`):\n",
        "          Device used to load tensors before feeding to model.\n",
        "\n",
        "  Returns:\n",
        "\n",
        "      :obj:`List[List[int], List[int], float]`: List of [True Labels, Predicted\n",
        "        Labels, Train Average Loss].\n",
        "  \"\"\"\n",
        "\n",
        "  # Use global variable for model.\n",
        "  global model\n",
        "\n",
        "  # Tracking variables.\n",
        "  predictions_labels = []\n",
        "  true_labels = []\n",
        "  # Total loss for this epoch.\n",
        "  total_loss = 0\n",
        "\n",
        "  # Put the model into training mode.\n",
        "  model.train()\n",
        "\n",
        "  # For each batch of training data...\n",
        "  for batch in tqdm(dataloader, total=len(dataloader)):\n",
        "\n",
        "    # Add original labels - use later for evaluation.\n",
        "    true_labels += batch['labels'].numpy().flatten().tolist()\n",
        "\n",
        "    # move batch to device\n",
        "    batch = {k:v.type(torch.long).to(device_) for k,v in batch.items()}\n",
        "\n",
        "    # Always clear any previously calculated gradients before performing a\n",
        "    # backward pass.\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Perform a forward pass (evaluate the model on this training batch).\n",
        "    # This will return the loss (rather than the model output) because we\n",
        "    # have provided the `labels`.\n",
        "    # The documentation for this a bert model function is here:\n",
        "    # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "    outputs = model(**batch)\n",
        "\n",
        "    # The call to `model` always returns a tuple, so we need to pull the\n",
        "    # loss value out of the tuple along with the logits. We will use logits\n",
        "    # later to calculate training accuracy.\n",
        "    loss, logits = outputs[:2]\n",
        "\n",
        "    # Accumulate the training loss over all of the batches so that we can\n",
        "    # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "    # single value; the `.item()` function just returns the Python value\n",
        "    # from the tensor.\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    # Perform a backward pass to calculate the gradients.\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip the norm of the gradients to 1.0.\n",
        "    # This is to help prevent the \"exploding gradients\" problem.\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    # Update parameters and take a step using the computed gradient.\n",
        "    # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "    # modified based on their gradients, the learning rate, etc.\n",
        "    optimizer_.step()\n",
        "\n",
        "    # Update the learning rate.\n",
        "    scheduler_.step()\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "\n",
        "    # Convert these logits to list of predicted labels values.\n",
        "    predictions_labels += logits.argmax(axis=-1).flatten().tolist()\n",
        "\n",
        "  # Calculate the average loss over the training data.\n",
        "  avg_epoch_loss = total_loss / len(dataloader)\n",
        "\n",
        "  # Return all true labels and prediction for future evaluations.\n",
        "  return true_labels, predictions_labels, avg_epoch_loss\n",
        "\n",
        "\n",
        "\n",
        "def validation(dataloader, device_):\n",
        "  r\"\"\"Validation function to evaluate model performance on a\n",
        "  separate set of data.\n",
        "\n",
        "  This function will return the true and predicted labels so we can use later\n",
        "  to evaluate the model's performance.\n",
        "\n",
        "  This function is built with reusability in mind: it can be used as is as long\n",
        "    as the `dataloader` outputs a batch in dictionary format that can be passed\n",
        "    straight into the model - `model(**batch)`.\n",
        "\n",
        "  Arguments:\n",
        "\n",
        "    dataloader (:obj:`torch.utils.data.dataloader.DataLoader`):\n",
        "          Parsed data into batches of tensors.\n",
        "\n",
        "    device_ (:obj:`torch.device`):\n",
        "          Device used to load tensors before feeding to model.\n",
        "\n",
        "  Returns:\n",
        "\n",
        "    :obj:`List[List[int], List[int], float]`: List of [True Labels, Predicted\n",
        "        Labels, Train Average Loss]\n",
        "  \"\"\"\n",
        "\n",
        "  # Use global variable for model.\n",
        "  global model\n",
        "\n",
        "  # Tracking variables\n",
        "  predictions_labels = []\n",
        "  true_labels = []\n",
        "  #total loss for this epoch.\n",
        "  total_loss = 0\n",
        "\n",
        "  # Put the model in evaluation mode--the dropout layers behave differently\n",
        "  # during evaluation.\n",
        "  model.eval()\n",
        "\n",
        "  # Evaluate data for one epoch\n",
        "  for batch in tqdm(dataloader, total=len(dataloader)):\n",
        "\n",
        "    # add original labels\n",
        "    true_labels += batch['labels'].numpy().flatten().tolist()\n",
        "\n",
        "    # move batch to device\n",
        "    batch = {k:v.type(torch.long).to(device_) for k,v in batch.items()}\n",
        "\n",
        "    # Telling the model not to compute or store gradients, saving memory and\n",
        "    # speeding up validation\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # Forward pass, calculate logit predictions.\n",
        "        # This will return the logits rather than the loss because we have\n",
        "        # not provided labels.\n",
        "        # token_type_ids is the same as the \"segment ids\", which\n",
        "        # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "        # The documentation for this `model` function is here:\n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(**batch)\n",
        "\n",
        "        # The call to `model` always returns a tuple, so we need to pull the\n",
        "        # loss value out of the tuple along with the logits. We will use logits\n",
        "        # later to to calculate training accuracy.\n",
        "        loss, logits = outputs[:2]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value\n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # get predicitons to list\n",
        "        predict_content = logits.argmax(axis=-1).flatten().tolist()\n",
        "\n",
        "        # update list\n",
        "        predictions_labels += predict_content\n",
        "\n",
        "  # Calculate the average loss over the training data.\n",
        "  avg_epoch_loss = total_loss / len(dataloader)\n",
        "\n",
        "  # Return all true labels and prediciton for future evaluations.\n",
        "  return true_labels, predictions_labels, avg_epoch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIJm2UAPd1v7"
      },
      "outputs": [],
      "source": [
        "# Get model configuration.\n",
        "print('Loading configuraiton...')\n",
        "model_config = GPT2Config.from_pretrained(pretrained_model_name_or_path=model_name_or_path, num_labels=n_labels)\n",
        "\n",
        "# Get model's tokenizer.\n",
        "print('Loading tokenizer...')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path=model_name_or_path)\n",
        "# default to left padding\n",
        "tokenizer.padding_side = \"left\"\n",
        "# Define PAD Token = EOS Token = 50256\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "# Get the actual model.\n",
        "print('Loading model...')\n",
        "model = GPT2ForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_name_or_path, config=model_config)\n",
        "\n",
        "# resize model embedding to match new tokenizer\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# fix model padding token id\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "# Load model to defined device.\n",
        "model.to(device)\n",
        "print('Model loaded to `%s`'%device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M578OB8-d1v8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('gold_std_clean.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y044mC9Gd1v8"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-RIXOeQd1v8"
      },
      "outputs": [],
      "source": [
        "df = df.rename(columns={'context': 'text'})\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XIM3_Cxd1v8"
      },
      "outputs": [],
      "source": [
        "# Mapping dictionary for label values\n",
        "label_mapping = {0: 'not relevant', 1: 'relevant'}\n",
        "\n",
        "# Replace label values using the mapping dictionary\n",
        "df['label'] = df['label'].map(label_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20nNfu97d1v8"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xy0pGTx3d1v8"
      },
      "outputs": [],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KmxLT8ed1v9"
      },
      "outputs": [],
      "source": [
        "#Creating a dataframe with 80% values of original dataframe\n",
        "train_df = df.sample(frac = 0.8)\n",
        "\n",
        "#Creating dataframe with rest of the 20% values\n",
        "test_df = df.drop(train_df.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhekUi2yd1v9"
      },
      "outputs": [],
      "source": [
        "print('train dataset: ', len(train_df))\n",
        "print('test dataset: ', len(train_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wEkRFqzd1v9"
      },
      "outputs": [],
      "source": [
        "train_df.to_csv('gold_std_clean_train.csv')\n",
        "train_df.to_csv('gold_std_clean_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wev2nZfd1v9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "'''\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.dataframe = dataframe\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.dataframe.iloc[index].to_numpy()\n",
        "        features = row[1:]\n",
        "        label = row[2]\n",
        "        return features, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2TML9cdd1v9"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "train_dataset = CustomDataset(dataframe=train_df)\n",
        "test_dataset = CustomDataset(dataframe=test_df)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSY0UkYqd1v9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34rUebYnd1v-"
      },
      "outputs": [],
      "source": [
        "\n",
        "'''\n",
        "# Assuming you already have a dataframe called `df` with columns 'text' and 'label' - make sure you split them into train and test before this so you can run it for both\n",
        "# and get 2 dictionaries - one for train and one for validation. Name them appropriately so you don't have to make much changes to the code.\n",
        "\n",
        "# Create empty lists for texts and labels\n",
        "texts = []\n",
        "labels = []\n",
        "\n",
        "# Iterate over each row in the dataframe\n",
        "for index, row in train_df.iterrows():\n",
        "    text = row['context']\n",
        "    label = row['label']\n",
        "\n",
        "    # Create a dictionary for the current row\n",
        "    example = {'text': text, 'label': label}\n",
        "\n",
        "    # Append the example to the lists\n",
        "    texts.append(example['text'])\n",
        "    labels.append(example['label'])\n",
        "\n",
        "# Create a dictionary with 'texts' and 'labels' lists\n",
        "train_dict = {'texts': texts, 'labels': labels}\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cy7fdXhmd1v-"
      },
      "outputs": [],
      "source": [
        "#train_dict['texts'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Z6rxQ9xd1v-"
      },
      "outputs": [],
      "source": [
        "#train_dict['labels'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTc5s2EAd1v_"
      },
      "outputs": [],
      "source": [
        "class MovieReviewsDataset(Dataset):\n",
        "    def __init__(self, dataframe, use_tokenizer):\n",
        "        self.texts = dataframe['text'].tolist()\n",
        "        self.labels = dataframe['label'].tolist()\n",
        "        self.n_examples = len(self.labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_examples\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return {'text': self.texts[item], 'label': self.labels[item]}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jj2UNaPid1v_"
      },
      "outputs": [],
      "source": [
        "# Create data collator to encode text and labels into numbers.\n",
        "gpt2_classificaiton_collator = Gpt2ClassificationCollator(use_tokenizer=tokenizer,\n",
        "                                                          labels_encoder=labels_ids,\n",
        "                                                          max_sequence_len=max_length)\n",
        "\n",
        "print('Dealing with Train...')\n",
        "# Create pytorch dataset. -- modify this for my use case\n",
        "train_dataset = MovieReviewsDataset(train_df,\n",
        "                               use_tokenizer=tokenizer)\n",
        "print('Created `train_dataset` with %d examples!'%len(train_dataset))\n",
        "\n",
        "\n",
        "# Move train dataset into dataloader.\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=gpt2_classificaiton_collator)\n",
        "print('Created `train_dataloader` with %d batches!'%len(train_dataloader))\n",
        "\n",
        "print()\n",
        "\n",
        "print('Dealing with Validation...')\n",
        "# Create pytorch dataset. -- modify this for my use case\n",
        "valid_dataset =  MovieReviewsDataset(test_df,\n",
        "                               use_tokenizer=tokenizer)\n",
        "print('Created `valid_dataset` with %d examples!'%len(valid_dataset))\n",
        "\n",
        "# Move test dataset into dataloader.\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=gpt2_classificaiton_collator)\n",
        "print('Created `eval_dataloader` with %d batches!'%len(valid_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9OxaP6zd1v_"
      },
      "outputs": [],
      "source": [
        "type(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21Ol2zEqd1v_"
      },
      "outputs": [],
      "source": [
        "type(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEbifeU2d1v_"
      },
      "outputs": [],
      "source": [
        "type(optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgIOIDQLd1wA"
      },
      "outputs": [],
      "source": [
        "type(scheduler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9Z0Ji0Pd1wA"
      },
      "outputs": [],
      "source": [
        "type(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nk8dEGqfd1wA"
      },
      "outputs": [],
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch)\n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # default is 1e-8.\n",
        "                  )\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "# `train_dataloader` contains batched data so `len(train_dataloader)` gives\n",
        "# us the number of batches.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuSzG4Kad1wA"
      },
      "outputs": [],
      "source": [
        "# Store the average loss after each epoch so we can plot them.\n",
        "all_loss = {'train_loss':[], 'val_loss':[]}\n",
        "all_acc = {'train_acc':[], 'val_acc':[]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViUDgf4Td1wA"
      },
      "outputs": [],
      "source": [
        "# Loop through each epoch.\n",
        "print('Epoch')\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print()\n",
        "  print('Training on batches...')\n",
        "  # Perform one full pass over the training set.\n",
        "  train_labels, train_predict, train_loss = train(train_dataloader, optimizer, scheduler, device)\n",
        "  train_acc = accuracy_score(train_labels, train_predict)\n",
        "\n",
        "  # Get prediction form model on validation data.\n",
        "  print('Validation on batches...')\n",
        "  valid_labels, valid_predict, val_loss = validation(valid_dataloader, device)\n",
        "  val_acc = accuracy_score(valid_labels, valid_predict)\n",
        "\n",
        "  # Print loss and accuracy values to see how training evolves.\n",
        "  print(\"  train_loss: %.5f - val_loss: %.5f - train_acc: %.5f - valid_acc: %.5f\"%(train_loss, val_loss, train_acc, val_acc))\n",
        "  print()\n",
        "\n",
        "  # Store the loss value for plotting the learning curve.\n",
        "  all_loss['train_loss'].append(train_loss)\n",
        "  all_loss['val_loss'].append(val_loss)\n",
        "  all_acc['train_acc'].append(train_acc)\n",
        "  all_acc['val_acc'].append(val_acc)\n",
        "\n",
        "# Plot loss curves.\n",
        "plot_dict(all_loss, use_xlabel='Epochs', use_ylabel='Value', use_linestyles=['-', '--'])\n",
        "\n",
        "# Plot accuracy curves.\n",
        "plot_dict(all_acc, use_xlabel='Epochs', use_ylabel='Value', use_linestyles=['-', '--'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWMvYDzVd1wA"
      },
      "outputs": [],
      "source": [
        "# Evaluate\n",
        "# Get prediction form model on validation data. This is where you should use\n",
        "# your test data.\n",
        "true_labels, predictions_labels, avg_epoch_loss = validation(valid_dataloader, device)\n",
        "\n",
        "# Create the evaluation report.\n",
        "evaluation_report = classification_report(true_labels, predictions_labels, labels=list(labels_ids.values()), target_names=list(labels_ids.keys()))\n",
        "# Show the evaluation report.\n",
        "print(evaluation_report)\n",
        "\n",
        "# Plot confusion matrix.\n",
        "plot_confusion_matrix(y_true=true_labels, y_pred=predictions_labels,\n",
        "                      classes=list(labels_ids.keys()), normalize=True,\n",
        "                      magnify=0.1,\n",
        "                      );"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HcIp1eUd1wB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aihYXVZ7d1wB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ug6mIzeH-Aq8",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "# Non-BERT Classifiers (Ignore)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVc0s48lJWjR"
      },
      "source": [
        "Non-BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRi0FJEHJXRq"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "try:\n",
        "    import xlrd\n",
        "except ImportError as e:\n",
        "    !pip install xlrd\n",
        "    import xlrd\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import warnings; warnings.simplefilter('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeY5Tq9HJXog"
      },
      "outputs": [],
      "source": [
        "# We'll use scikit-learn for the classification algorithms.\n",
        "# https://scikit-learn.org/stable/\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYhnLi8tJZrT"
      },
      "outputs": [],
      "source": [
        "## sklearn also has some nice funtions for representations\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "## and for evaluation\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix, precision_score, recall_score #JC - look at More non-BERT section\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score #JC - look at More non-BERT section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlO58HsmwCi8"
      },
      "outputs": [],
      "source": [
        "!pip install seaborn\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCS7Ll94JbKD"
      },
      "outputs": [],
      "source": [
        "## Since we're working with text, we might need to tokenize for some of these representations.\n",
        "# We'll use nltk here, but there are other nlp packages available for this\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkxdJpkTJcgf"
      },
      "outputs": [],
      "source": [
        "# You have also learnt about embedding representations. These can also be used for classification.\n",
        "# We will use a library called Zeugma, which allows using pre-trained embedding models\n",
        "#Zeugma library: https://github.com/nkthiebaut/zeugma\n",
        "\n",
        "try:\n",
        "    from zeugma.embeddings import EmbeddingTransformer\n",
        "except ImportError as e:\n",
        "    !pip install zeugma\n",
        "    !pip install theano\n",
        "    from zeugma.embeddings import EmbeddingTransformer\n",
        "\n",
        "from datetime import datetime\n",
        "print(datetime.now())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fUS2trnwLC7"
      },
      "outputs": [],
      "source": [
        "training_data['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4tlr283wT4y"
      },
      "outputs": [],
      "source": [
        "sns.countplot(training_data['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOEo6ayxwcnX"
      },
      "outputs": [],
      "source": [
        "trainingdata, valdata, testdata = np.split(data_for_classifier.sample(frac=1), [int(.6*len(data_for_classifier)), int(.8*len(data_for_classifier))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yDyEhsIwxlR"
      },
      "outputs": [],
      "source": [
        "print(\"Training data: \", trainingdata.shape)\n",
        "print(\"Test data: \", testdata.shape)\n",
        "print(\"Validation data: \", valdata.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PNKXNFbJekB"
      },
      "outputs": [],
      "source": [
        "#training_data_example = training_data['text'].tolist()[0]\n",
        "training_data_example = trainingdata['context'].tolist()[12]\n",
        "print(training_data_example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MekoG8e97cE",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "# BoW Representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPmxRp8jJsPB"
      },
      "source": [
        "BoW Representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6yQxNOCJocn"
      },
      "outputs": [],
      "source": [
        "first_vectorizer = CountVectorizer(ngram_range=(1,1), stop_words=None,\n",
        "                             tokenizer=word_tokenize, max_features=500)\n",
        "#first_vectorizer.fit(training_data['text'].tolist())\n",
        "first_vectorizer.fit(training_data['context'].values.astype('U').tolist())\n",
        "#first_vectorizer.fit(trainingdata['text'].values.astype('U').tolist())\n",
        "#first_fit_transformed_data = first_vectorizer.fit_transform(training_data['text'])\n",
        "first_fit_transformed_data = first_vectorizer.fit_transform(training_data['context'].values.astype('U'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pao7k2B4JylH"
      },
      "outputs": [],
      "source": [
        "first_transformed_data = first_vectorizer.transform([training_data['context'].tolist()[231]])\n",
        "print (first_transformed_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-1hxdVPJ307"
      },
      "outputs": [],
      "source": [
        "print (first_vectorizer.get_feature_names()[377])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlvO4AR6J7MT"
      },
      "outputs": [],
      "source": [
        "print(first_fit_transformed_data.shape)\n",
        "print ('Amount of Non-Zero occurences: ', first_fit_transformed_data.nnz)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZzMKWqRJ-XD"
      },
      "source": [
        "Build a classifier with this feature representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR3xmg66KJON"
      },
      "source": [
        "K-nearest Neighbour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQdV8OEJJ_2I"
      },
      "outputs": [],
      "source": [
        "kneighbour_classifier = KNeighborsClassifier().fit(first_fit_transformed_data, training_data['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yczz7I7IKLwr"
      },
      "outputs": [],
      "source": [
        "## We need to transform test data to the same representation\n",
        "first_fit_transformed_testdata = first_vectorizer.transform(testing_data['context'].values.astype('U'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Svb3lCFDKU96"
      },
      "outputs": [],
      "source": [
        "first_fit_transformed_testdata\n",
        "kneighbour_predicted = kneighbour_classifier.predict(first_fit_transformed_testdata)\n",
        "kneighbour_predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgB3owdhKXRj"
      },
      "outputs": [],
      "source": [
        "print(metrics.classification_report(testing_data['label'], kneighbour_predicted, target_names=set(testing_data['label'].values.astype('U').tolist())))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bODBU6Y-KsBi"
      },
      "source": [
        "We can employ n-fold cross-validation on the training data to experiment with different representations, parameters, and classifiers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFxYdq93Kbl4"
      },
      "outputs": [],
      "source": [
        "kneighbour_classifier = KNeighborsClassifier().fit(first_fit_transformed_data, training_data['label'])\n",
        "scoring = ['precision_macro', 'recall_macro','precision_micro','recall_micro', 'f1_micro', 'f1_macro']\n",
        "scores = cross_validate(kneighbour_classifier, first_fit_transformed_data, training_data['label'], scoring=scoring, cv=10, return_train_score=False)\n",
        "scoresdf = pd.DataFrame(scores)\n",
        "scoring = ['test_precision_macro', 'test_recall_macro','test_precision_micro','test_recall_micro', 'test_f1_micro', 'test_f1_macro']\n",
        "bp = scoresdf.boxplot(column=scoring, grid=False, rot=45,)\n",
        "[ax_tmp.set_xlabel('') for ax_tmp in np.asarray(bp).reshape(-1)]\n",
        "fig = np.asarray(bp).reshape(-1)[0].get_figure()\n",
        "fig.suptitle('K nearest neighbour, count vectorizer')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwlRDX9md1wG"
      },
      "outputs": [],
      "source": [
        "scoresdf.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaGAmYffK0ik"
      },
      "source": [
        "Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MJLUX_dKwSC"
      },
      "outputs": [],
      "source": [
        "rf_classifier = RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0).fit(first_fit_transformed_data, training_data['label'])\n",
        "scoring = ['precision_macro', 'recall_macro','precision_micro','recall_micro', 'f1_micro', 'f1_macro']\n",
        "scores = cross_validate(rf_classifier, first_fit_transformed_data, training_data['label'], scoring=scoring, cv=10, return_train_score=False)\n",
        "scoresdf = pd.DataFrame(scores)\n",
        "scoring = ['test_precision_macro', 'test_recall_macro','test_precision_micro','test_recall_micro', 'test_f1_micro', 'test_f1_macro']\n",
        "bp = scoresdf.boxplot(column=scoring, grid=False, rot=45,)\n",
        "[ax_tmp.set_xlabel('') for ax_tmp in np.asarray(bp).reshape(-1)]\n",
        "fig = np.asarray(bp).reshape(-1)[0].get_figure()\n",
        "fig.suptitle('Random forest, count vectorizer')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uZaA6PO90gm",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "# TF-IDF Representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyS49kcZK-E_"
      },
      "source": [
        "Tf-idf representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gnPEEKEK5hj"
      },
      "outputs": [],
      "source": [
        "stopWords = set(stopwords.words('english'))\n",
        "tfidf_vect = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stopWords)\n",
        "tfidf_vect.fit(training_data['context'])\n",
        "second_fit_transformed_data =  tfidf_vect.transform(training_data['context'])\n",
        "second_fit_transformed_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_Q8Fn0iLJsM"
      },
      "source": [
        "Use this with Multinomial Naive Bayes classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69VMo2lcLEtJ"
      },
      "outputs": [],
      "source": [
        "kneighbour_classifier = KNeighborsClassifier().fit(second_fit_transformed_data, training_data['label'])\n",
        "scoring = ['precision_macro', 'recall_macro','precision_micro','recall_micro', 'f1_micro', 'f1_macro']\n",
        "scores = cross_validate(kneighbour_classifier, second_fit_transformed_data, training_data['label'], scoring=scoring, cv=10, return_train_score=False)\n",
        "scoresdf = pd.DataFrame(scores)\n",
        "scoring = ['test_precision_macro', 'test_recall_macro','test_precision_micro','test_recall_micro', 'test_f1_micro', 'test_f1_macro']\n",
        "bp = scoresdf.boxplot(column=scoring, grid=False, rot=45,)\n",
        "[ax_tmp.set_xlabel('') for ax_tmp in np.asarray(bp).reshape(-1)]\n",
        "fig = np.asarray(bp).reshape(-1)[0].get_figure()\n",
        "fig.suptitle('K nearest neighbour, tf-idf vectorizer')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCC2meSE9wdv",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "# Embedding Representations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL5Q70BfLRPB"
      },
      "source": [
        "Embedding Representations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKp3TEoVLOI-"
      },
      "outputs": [],
      "source": [
        "glove = EmbeddingTransformer('glove')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeWHWxI9LUhf"
      },
      "outputs": [],
      "source": [
        "glove_transformed_training_data = glove.transform(training_data['context'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osAi7WoILXSr"
      },
      "outputs": [],
      "source": [
        "glove_transformed_training_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd8OkYddLa3D"
      },
      "source": [
        "Build a classifier with this representation and evaluate with 10-fold cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0R3UcQpLYts"
      },
      "outputs": [],
      "source": [
        "kneighbour_classifier = KNeighborsClassifier().fit(glove_transformed_training_data, training_data['label'])\n",
        "scoring = ['precision_macro', 'recall_macro','precision_micro','recall_micro', 'f1_micro', 'f1_macro']\n",
        "scores = cross_validate(kneighbour_classifier, glove_transformed_training_data, training_data['label'], scoring=scoring, cv=10, return_train_score=False)\n",
        "scoresdf = pd.DataFrame(scores)\n",
        "scoring = ['test_precision_macro', 'test_recall_macro','test_precision_micro','test_recall_micro', 'test_f1_micro', 'test_f1_macro']\n",
        "bp = scoresdf.boxplot(column=scoring, grid=False, rot=45,)\n",
        "[ax_tmp.set_xlabel('') for ax_tmp in np.asarray(bp).reshape(-1)]\n",
        "fig = np.asarray(bp).reshape(-1)[0].get_figure()\n",
        "fig.suptitle('K nearest neighbour, pre-trained embeddings')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOzqD0ZN9qSt",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "# Different configurations in one go"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP_CrjGZLibj"
      },
      "source": [
        "Some different configurations all in one go"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lcm02b-Leap"
      },
      "outputs": [],
      "source": [
        "\n",
        "representations = {}\n",
        "\n",
        "vectorizer = CountVectorizer(ngram_range=(1,1), stop_words=None,\n",
        "                             tokenizer=word_tokenize, max_features=500)\n",
        "xtrain_countvect = vectorizer.fit_transform(training_data['context'])\n",
        "representations['CountVectorizer'] = xtrain_countvect\n",
        "\n",
        "tfidf_vect = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stopWords)\n",
        "tfidf_vect.fit(training_data['context'])\n",
        "xtrain_tfidf =  tfidf_vect.transform(training_data['context'])\n",
        "representations['TfidfVectorizer'] = xtrain_tfidf\n",
        "\n",
        "x_train_glove = glove.transform(training_data['context'])\n",
        "representations['pretrained_glove'] = x_train_glove\n",
        "\n",
        "\n",
        "\n",
        "CV = 10\n",
        "\n",
        "classifier_models = [\n",
        "        RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
        "        DecisionTreeClassifier(),\n",
        "        SVC(),\n",
        "        LinearSVC(multi_class='ovr', C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
        "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
        "      penalty='l2', random_state=0, tol=1e-05, verbose=0),\n",
        "        SGDClassifier(),\n",
        "        LogisticRegression(random_state=0),\n",
        "        KNeighborsClassifier(),\n",
        "]\n",
        "\n",
        "cv_df = pd.DataFrame(index=range(CV * (len(classifier_models)*len(representations))))\n",
        "entries = []\n",
        "\n",
        "\n",
        "for representation, transformed_vector in representations.items():\n",
        "    score = 'f1_micro'\n",
        "    for model in classifier_models:\n",
        "      model_name = model.__class__.__name__+'_'+representation\n",
        "      accuracies = cross_val_score(model, transformed_vector, training_data['label'], scoring=score, cv=CV)\n",
        "      for fold_idx, accuracy in enumerate(accuracies):\n",
        "        entries.append((model_name, fold_idx, accuracy))\n",
        "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', score])\n",
        "bp = cv_df.boxplot(by='model_name', column=[score], grid=False, rot=90, figsize=(12,8))\n",
        "[ax_tmp.set_xlabel('') for ax_tmp in np.asarray(bp).reshape(-1)]\n",
        "fig = np.asarray(bp).reshape(-1)[0].get_figure()\n",
        "fig.suptitle('10-fold cross validation results')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNcnFOET9mKl",
        "tags": []
      },
      "source": [
        "# More experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gN3q-tpDLvPJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "## First step: Transform your training and test data to your chosen representation.\n",
        "\n",
        "## choose a representation: CountVectorizer or TfidfVectorizer, or embeddings\n",
        "\n",
        "chosen_representation = tfidf_vect\n",
        "\n",
        "## transform the training data\n",
        "transformed_training_data = chosen_representation.transform(training_data['context'])\n",
        "\n",
        "## transform the test data\n",
        "transformed_test_data = chosen_representation.transform(testing_data['context'])\n",
        "\n",
        "## Second step: Create a classifier - the one you think gave best results when experimenting with cross-validation\n",
        "\n",
        "\n",
        "chosen_classifier = LinearSVC(multi_class='ovr', C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
        "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
        "      penalty='l2', random_state=0, tol=1e-05, verbose=0)\n",
        "\n",
        "## train the classifier on the training data\n",
        "chosen_classifier.fit(transformed_training_data, training_data['label'])\n",
        "\n",
        "## predict labels on the test data\n",
        "predicted = chosen_classifier.predict(transformed_test_data)\n",
        "\n",
        "## what results do you get? Note that you can look at both macro and micro scores!\n",
        "print(metrics.classification_report(testing_data['label'], predicted, target_names=set(testing_data['label'].values.astype('U').tolist())))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MU7-3zS39enU",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "# Confidence Intervals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4FQCbuV9CgF"
      },
      "source": [
        "Get confidence intervals - https://sebastianraschka.com/blog/2022/confidence-intervals-for-ml.html\n",
        "\n",
        "code - https://gist.github.com/roncho12/60178f12ea4c3a74764fd645c6f2fe13\n",
        "\n",
        "https://towardsdatascience.com/how-to-add-confidence-intervals-to-any-model-7bbb9f80fd9c\n",
        "\n",
        "\n",
        "https://sebastianraschka.com/blog/2022/confidence-intervals-for-ml.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKQiQZuV89OF"
      },
      "outputs": [],
      "source": [
        "import scipy.stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBsg3MIOXXSi"
      },
      "source": [
        "Bootstrapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ualxwq3XZFq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcjGmertXZCH"
      },
      "outputs": [],
      "source": [
        "#From Diana\n",
        "\n",
        "def f1_conf_int(target, predict, bootstrap_n=500):\n",
        "    n = len(target)\n",
        "    f1 = np.zeros(bootstrap_n)\n",
        "    for bs in np.arange(bootstrap_n):\n",
        "        i = np.random.choice(n, n, replace=True)\n",
        "        tp = pd.DataFrame({\"t\":target, \"p\":predict})\n",
        "        tp_bootstrap = tp.iloc[i,:]\n",
        "        f1[bs] = (f1_score(tp_bootstrap[\"t\"], tp_bootstrap[\"p\"]))\n",
        "    res = [f1.mean(), np.quantile(f1,0.025), np.quantile(f1,0.975), f1.std()] #use these numbers for 95% CI\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuAwCytSXY-V"
      },
      "outputs": [],
      "source": [
        "def precision_conf_int(target, predict, bootstrap_n=500):\n",
        "    n = len(target)\n",
        "    precision = np.zeros(bootstrap_n)\n",
        "    for bs in np.arange(bootstrap_n):\n",
        "        i = np.random.choice(n, n, replace=True)\n",
        "        tp = pd.DataFrame({\"t\":target, \"p\":predict})\n",
        "        tp_bootstrap = tp.iloc[i,:]\n",
        "        precision[bs] = (precision_score(tp_bootstrap[\"t\"], tp_bootstrap[\"p\"]))\n",
        "    res = [precision.mean(), np.quantile(precision,0.025), np.quantile(precision,0.975), precision.std()]#use these numbers for 95% CI\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hRUnV5sXY1v"
      },
      "outputs": [],
      "source": [
        "def recall_conf_int(target, predict, bootstrap_n=500):\n",
        "    n = len(target)\n",
        "    recall = np.zeros(bootstrap_n)\n",
        "    for bs in np.arange(bootstrap_n):\n",
        "        i = np.random.choice(n, n, replace=True)\n",
        "        tp = pd.DataFrame({\"t\":target, \"p\":predict})\n",
        "        tp_bootstrap = tp.iloc[i,:]\n",
        "        recall[bs] = (recall_score(tp_bootstrap[\"t\"], tp_bootstrap[\"p\"]))\n",
        "    res = [recall.mean(), np.quantile(recall,0.025), np.quantile(recall,0.975), recall.std()]#use these numbers for 95% CI\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoqNpyTUd1wK"
      },
      "outputs": [],
      "source": [
        "bootstrap_n=100\n",
        "bootstrap_n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VORlc_sGd1wK"
      },
      "outputs": [],
      "source": [
        "recall = np.zeros(bootstrap_n)\n",
        "recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DMz2NHFd1wK"
      },
      "outputs": [],
      "source": [
        "np.quantile(recall,0.94)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2_3vJYod1wK"
      },
      "outputs": [],
      "source": [
        "100*0.06"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUJxh1mW9XF1",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "# Predict on unseen random text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RolC9_oWMHQi"
      },
      "source": [
        "Predict unseen random text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pI8pvZo_MJ5Q"
      },
      "outputs": [],
      "source": [
        "new_text = 'Patient with severe depression and heart pain.'\n",
        "testX = chosen_representation.transform([new_text])\n",
        "predicted = chosen_classifier.predict(testX)\n",
        "print(predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5AoCLaVMNGY"
      },
      "outputs": [],
      "source": [
        "new_text = '5-year old girl with asthma.'\n",
        "testX = chosen_representation.transform([new_text])\n",
        "predicted = chosen_classifier.predict(testX)\n",
        "print(predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bitghhh7MSOX"
      },
      "outputs": [],
      "source": [
        "new_text = 'Her asthma was getting better'\n",
        "testX = chosen_representation.transform([new_text])\n",
        "predicted = chosen_classifier.predict(testX)\n",
        "print(predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWaUw11RMSkw"
      },
      "outputs": [],
      "source": [
        "new_text = 'that movie was amazing.'\n",
        "testX = chosen_representation.transform([new_text])\n",
        "predicted = chosen_classifier.predict(testX)\n",
        "print(predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ow-gVDcVNXeO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gtr0tNXjttBM",
        "tags": []
      },
      "source": [
        "# More non-BERT classifiers - use these"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byBk0jajtzS_"
      },
      "source": [
        "https://github.com/vijayaiitk/NLP-text-classification-model/blob/main/NLP%20text%20classification%20model%20Github.ipynb\n",
        "\n",
        "\n",
        "https://medium.com/analytics-vidhya/nlp-tutorial-for-text-classification-in-python-8f19cd17b49e#:~:text=Text%20classification%20is%20one%20of,in%20a%20cost%2Deffective%20manner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zobuRGpJd1wM"
      },
      "outputs": [],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjAn1LfLd1wM"
      },
      "outputs": [],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoIGM-d8txmJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#for text pre-processing\n",
        "import re, string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "#for model-building\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC #JC\n",
        "from sklearn.ensemble import RandomForestClassifier #JC\n",
        "from sklearn.neighbors import KNeighborsClassifier #JC\n",
        "from sklearn.naive_bayes import GaussianNB #JC\n",
        "from sklearn.tree import DecisionTreeClassifier #JC\n",
        "from sklearn.svm import SVC #JC\n",
        "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix, precision_score, recall_score\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "\n",
        "# bag of words\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer #JC\n",
        "\n",
        "#for word embedding\n",
        "import gensim\n",
        "from gensim.models import Word2Vec #Word2Vec is mostly used for huge datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkS76ysQt3HA"
      },
      "outputs": [],
      "source": [
        "#you can download the data from https://www.kaggle.com/c/nlp-getting-started/data\n",
        "import os\n",
        "#os.chdir('/Users/ranivija/Desktop/')\n",
        "#df_train=pd.read_csv('train.csv')\n",
        "#df_train=pd.read_csv('gdrive/My Drive/sample_size/5000_data_for_classifier.csv')#5000 docs\n",
        "\n",
        "df_train = training_data\n",
        "print(df_train.shape)\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDDT9koet40X"
      },
      "outputs": [],
      "source": [
        "# CLASS DISTRIBUTION\n",
        "#if dataset is balanced or not\n",
        "#x=df_train['target'].value_counts()\n",
        "x=df_train['label'].value_counts()\n",
        "print(x)\n",
        "sns.barplot(x.index,x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z64ndritt8AA"
      },
      "outputs": [],
      "source": [
        "#Missing values\n",
        "df_train.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tp6QhS7mt-UL"
      },
      "outputs": [],
      "source": [
        "#1. WORD-COUNT\n",
        "df_train['word_count'] = df_train['context'].apply(lambda x: len(str(x).split()))\n",
        "#print(df_train[df_train['target']==1]['word_count'].mean()) #Disaster tweets\n",
        "#print(df_train[df_train['target']==0]['word_count'].mean()) #Non-Disaster tweets\n",
        "#Disaster tweets are more wordy than the non-disaster tweets\n",
        "print(\"Relevant word count: \", df_train[df_train['label']==1]['word_count'].mean()) #HTN\n",
        "print(\"Not relevant word count: \", df_train[df_train['label']==0]['word_count'].mean()) #Non-HTN\n",
        "\n",
        "#2. CHARACTER-COUNT\n",
        "df_train['char_count'] = df_train['context'].apply(lambda x: len(str(x)))\n",
        "#print(df_train[df_train['target']==1]['char_count'].mean()) #Disaster tweets\n",
        "#print(df_train[df_train['target']==0]['char_count'].mean()) #Non-Disaster tweets\n",
        "#Disaster tweets are longer than the non-disaster tweets\n",
        "print(\"Relevant char count: \",df_train[df_train['label']==1]['char_count'].mean()) #HTN\n",
        "print(\"Not relevant char count: \",df_train[df_train['label']==0]['char_count'].mean()) #Non-HTN\n",
        "\n",
        "#3. UNIQUE WORD-COUNT\n",
        "df_train['unique_word_count'] = df_train['context'].apply(lambda x: len(set(str(x).split())))\n",
        "#print(df_train[df_train['target']==1]['unique_word_count'].mean()) #Disaster tweets\n",
        "#print(df_train[df_train['target']==0]['unique_word_count'].mean()) #Non-Disaster tweets\n",
        "print(\"Relevant unique word count: \",df_train[df_train['label']==1]['unique_word_count'].mean()) #HTN\n",
        "print(\"Not relevant unique word count: \",df_train[df_train['label']==0]['unique_word_count'].mean()) #Non-HTN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmJ-E3F7t-R1"
      },
      "outputs": [],
      "source": [
        "#Plotting word-count per tweet\n",
        "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,4))\n",
        "#train_words=df_train[df_train['target']==1]['word_count']\n",
        "train_words=df_train[df_train['label']==1]['word_count']\n",
        "ax1.hist(train_words,color='red')\n",
        "#ax1.set_title('Disaster Tweets')\n",
        "ax1.set_title('Relevant')\n",
        "#train_words=df_train[df_train['target']==0]['word_count']\n",
        "train_words=df_train[df_train['label']==0]['word_count']\n",
        "ax2.hist(train_words,color='green')\n",
        "#ax2.set_title('Non-Disaster Tweets')\n",
        "ax2.set_title('Not relevant')\n",
        "#fig.suptitle('Words per tweet')\n",
        "fig.suptitle('Words per document')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqCTied9t-PO"
      },
      "outputs": [],
      "source": [
        "#1. Common text preprocessing\n",
        "text = \"   This is a message to be cleaned. It may involve some things like: <br>, ?, :, ''  adjacent spaces and tabs     .  \"\n",
        "\n",
        "#convert to lowercase and remove punctuations and characters and then strip\n",
        "def preprocess(text):\n",
        "    text = text.lower() #lowercase text\n",
        "    text=text.strip()  #get rid of leading/trailing whitespace\n",
        "    text=re.compile('<.*?>').sub('', text) #Remove HTML tags/markups\n",
        "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  #Replace punctuation with space. Careful since punctuation can sometime be useful\n",
        "    text = re.sub('\\s+', ' ', text)  #Remove extra space and tabs\n",
        "    text = re.sub(r'\\[[0-9]*\\]',' ',text) #[0-9] matches any digit (0 to 10000...)\n",
        "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
        "    text = re.sub(r'\\d',' ',text) #matches any digit from 0 to 100000..., \\D matches non-digits\n",
        "    text = re.sub(r'\\s+',' ',text) #\\s matches any whitespace, \\s+ matches multiple whitespace, \\S matches non-whitespace\n",
        "\n",
        "    return text\n",
        "\n",
        "text=preprocess(text)\n",
        "print(text)  #text is a string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_POxi5M9t-M9"
      },
      "outputs": [],
      "source": [
        "#3. LEXICON-BASED TEXT PROCESSING EXAMPLES\n",
        "\n",
        "#1. STOPWORD REMOVAL\n",
        "def stopword(string):\n",
        "    a= [i for i in string.split() if i not in stopwords.words('english')]\n",
        "    return ' '.join(a)\n",
        "\n",
        "text=stopword(text)\n",
        "print(text)\n",
        "\n",
        "#2. STEMMING\n",
        "\n",
        "# Initialize the stemmer\n",
        "snow = SnowballStemmer('english')\n",
        "def stemming(string):\n",
        "    a=[snow.stem(i) for i in word_tokenize(string) ]\n",
        "    return \" \".join(a)\n",
        "text=stemming(text)\n",
        "print(text)\n",
        "\n",
        "#3. LEMMATIZATION\n",
        "# Initialize the lemmatizer\n",
        "wl = WordNetLemmatizer()\n",
        "\n",
        "# This is a helper function to map NTLK position tags\n",
        "# Full list is available here: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "# Tokenize the sentence\n",
        "def lemmatizer(string):\n",
        "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
        "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
        "    return \" \".join(a)\n",
        "\n",
        "text = lemmatizer(text)\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zSqNXhmt-Kp"
      },
      "outputs": [],
      "source": [
        "#FINAL PREPROCESSING\n",
        "def finalpreprocess(string):\n",
        "    return lemmatizer(stopword(preprocess(string)))\n",
        "\n",
        "df_train['clean_text'] = df_train['context'].apply(lambda x: finalpreprocess(x)) #context instead of text\n",
        "df_train=df_train.drop(columns=['word_count','char_count','unique_word_count'])\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKsi5fOit-IN"
      },
      "outputs": [],
      "source": [
        "# create Word2vec model\n",
        "#here words_f should be a list containing words from each document. say 1st row of the list is words from the 1st document/sentence\n",
        "#length of words_f is number of documents/sentences in your dataset\n",
        "df_train['clean_text_tok']=[nltk.word_tokenize(i) for i in df_train['clean_text']] #convert preprocessed sentence to tokenized sentence\n",
        "model = Word2Vec(df_train['clean_text_tok'],min_count=1)  #min_count=1 means word should be present at least across all documents,\n",
        "#if min_count=2 means if the word is present less than 2 times across all the documents then we shouldn't consider it\n",
        "\n",
        "##The index2word attribute has been replaced by index_to_key since Gensim 4.0.0.\n",
        "##model.wv.syn0 informs about \" DeprecationWarning: Call to deprecated syn0 (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\" -- i used model.wv.vectors\n",
        "w2v = dict(zip(model.wv.index_to_key, model.wv.vectors))  #combination of word and its vector\n",
        "\n",
        "#for converting sentence to vectors/numbers from word vectors result by Word2Vec\n",
        "class MeanEmbeddingVectorizer(object):\n",
        "    def __init__(self, word2vec):\n",
        "        self.word2vec = word2vec\n",
        "        # if a text is empty we should return a vector of zeros\n",
        "        # with the same dimensionality as all the other vectors\n",
        "        self.dim = len(next(iter(word2vec.values())))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.array([\n",
        "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
        "                    or [np.zeros(self.dim)], axis=0)\n",
        "            for words in X\n",
        "        ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQuSmNQWt-FG"
      },
      "outputs": [],
      "source": [
        "#SPLITTING THE TRAINING DATASET INTO TRAINING AND VALIDATION\n",
        "\n",
        "# Input: \"reviewText\", \"rating\" and \"time\"\n",
        "# Target: \"log_votes\"\n",
        "X_train, X_val, y_train, y_val = train_test_split(df_train[\"clean_text\"],\n",
        "                                                  #df_train[\"target\"],\n",
        "                                                  df_train[\"label\"],\n",
        "                                                  test_size=0.2,\n",
        "                                                  shuffle=True)\n",
        "X_train_tok= [nltk.word_tokenize(i) for i in X_train]  #for word2vec\n",
        "X_val_tok= [nltk.word_tokenize(i) for i in X_val]      #for word2vec\n",
        "\n",
        "#TF-IDF\n",
        "# Convert x_train to vector since model can only run on numbers and not words- Fit and transform\n",
        "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
        "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) #tfidf runs on non-tokenized sentences unlike word2vec\n",
        "# Only transform x_test (not fit and transform)\n",
        "X_val_vectors_tfidf = tfidf_vectorizer.transform(X_val) #Don't fit() your TfidfVectorizer to your test data: it will\n",
        "#change the word-indexes & weights to match test data. Rather, fit on the training data, then use the same train-data-\n",
        "#fit model on the test data, to reflect the fact you're analyzing the test data only based on what was learned without\n",
        "#it, and the have compatible\n",
        "\n",
        "\n",
        "#Word2vec\n",
        "# Fit and transform\n",
        "modelw = MeanEmbeddingVectorizer(w2v)\n",
        "X_train_vectors_w2v = modelw.transform(X_train_tok)\n",
        "X_val_vectors_w2v = modelw.transform(X_val_tok)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fF2t3YcRuKi_"
      },
      "outputs": [],
      "source": [
        "#FITTING THE CLASSIFICATION MODEL using Logistic Regression(tf-idf)\n",
        "\n",
        "lr_tfidf=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')\n",
        "lr_tfidf.fit(X_train_vectors_tfidf, y_train)  #model\n",
        "\n",
        "#Predict y value for test dataset\n",
        "y_predict = lr_tfidf.predict(X_val_vectors_tfidf)\n",
        "y_prob = lr_tfidf.predict_proba(X_val_vectors_tfidf)[:,1]\n",
        "\n",
        "print('Logistic regression + tf-idf')\n",
        "print(classification_report(y_val,y_predict))\n",
        "print('Confusion Matrix:',confusion_matrix(y_val, y_predict))\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_val, y_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "print('AUC:', roc_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7utC7fXd1wP"
      },
      "outputs": [],
      "source": [
        "f1_conf_int(y_val,y_predict) #gives f1 mean, ci lower, ci upper, std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDbSep9gd1wP"
      },
      "outputs": [],
      "source": [
        "list1 = ['mean', 'ci_lower', 'ci_upper' , 'std']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdHmY1qld1wQ"
      },
      "outputs": [],
      "source": [
        "lrtfidf_f1 = pd.DataFrame(f1_conf_int(y_val, y_predict), index =list1,\n",
        "                                              columns =['val'])\n",
        "lrtfidf_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAvmKCGSd1wQ"
      },
      "outputs": [],
      "source": [
        "precision_conf_int(y_val, y_predict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLBz20UNd1wQ"
      },
      "outputs": [],
      "source": [
        "lrtfidf_p = pd.DataFrame(precision_conf_int(y_val, y_predict), index =list1,\n",
        "                                              columns =['val'])\n",
        "lrtfidf_p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUids8THd1wQ"
      },
      "outputs": [],
      "source": [
        "recall_conf_int(y_val, y_predict)\n",
        "\n",
        "lrtfidf_r = pd.DataFrame(recall_conf_int(y_val, y_predict), index =list1,\n",
        "                                              columns =['val'])\n",
        "lrtfidf_r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzGwMv16d1wQ"
      },
      "outputs": [],
      "source": [
        "lr_tfidf_report = classification_report(y_val,y_predict, output_dict=True)\n",
        "lrtfidf = pd.DataFrame(lr_tfidf_report).transpose()\n",
        "lrtfidf.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ljg1Fumxd1wR"
      },
      "outputs": [],
      "source": [
        "auc_list=[]\n",
        "\n",
        "auc_list.append(roc_auc)\n",
        "lrtfidf_auc = pd.DataFrame(auc_list, columns=['auc'])\n",
        "lrtfidf_auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJf8FPN2uKgq"
      },
      "outputs": [],
      "source": [
        "#FITTING THE CLASSIFICATION MODEL using Naive Bayes(tf-idf)\n",
        "#It's a probabilistic classifier that makes use of Bayes' Theorem, a rule that uses probability to make predictions based on prior knowledge of conditions that might be related. This algorithm is the most suitable for such large dataset as it considers each feature independently, calculates the probability of each category, and then predicts the category with the highest probability.\n",
        "\n",
        "\n",
        "nb_tfidf = MultinomialNB()\n",
        "nb_tfidf.fit(X_train_vectors_tfidf, y_train)  #model\n",
        "\n",
        "#Predict y value for test dataset\n",
        "y_predict = nb_tfidf.predict(X_val_vectors_tfidf)\n",
        "y_prob = nb_tfidf.predict_proba(X_val_vectors_tfidf)[:,1]\n",
        "\n",
        "print('NB + tf-idf')\n",
        "print(classification_report(y_val,y_predict))\n",
        "print('Confusion Matrix:',confusion_matrix(y_val, y_predict))\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_val, y_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "print('AUC:', roc_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4vdhVUdd1wR"
      },
      "outputs": [],
      "source": [
        "f1_conf_int(y_val,y_predict) #gives f1 mean, ci lower, ci upper, std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHq7fvm3d1wR"
      },
      "outputs": [],
      "source": [
        "nbtfidf_f1 = pd.DataFrame(f1_conf_int(y_val, y_predict), index =list1,\n",
        "                                              columns =['val'])\n",
        "\n",
        "nbtfidf_p = pd.DataFrame(precision_conf_int(y_val, y_predict), index =list1,\n",
        "                                              columns =['val'])\n",
        "\n",
        "nbtfidf_r = pd.DataFrame(recall_conf_int(y_val, y_predict), index =list1,\n",
        "                                              columns =['val'])\n",
        "\n",
        "nb_tfidf_report = classification_report(y_val,y_predict, output_dict=True)\n",
        "nbtfidf = pd.DataFrame(nb_tfidf_report).transpose()\n",
        "\n",
        "auc_list=[]\n",
        "\n",
        "auc_list.append(roc_auc)\n",
        "nbtfidf_auc = pd.DataFrame(auc_list, columns=['auc'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZeWASY7DHIa"
      },
      "outputs": [],
      "source": [
        "#FITTING THE CLASSIFICATION MODEL using Random Forest (tf-idf) ---- try\n",
        "\n",
        "#rf_tfidf = RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0)\n",
        "rf_tfidf = RandomForestClassifier()\n",
        "rf_tfidf.fit(X_train_vectors_tfidf, y_train)  #model\n",
        "\n",
        "#Predict y value for test dataset\n",
        "y_predict = rf_tfidf.predict(X_val_vectors_tfidf)\n",
        "y_prob = rf_tfidf.predict_proba(X_val_vectors_tfidf)[:,1]\n",
        "\n",
        "print('RF + tf-idf')\n",
        "print(classification_report(y_val,y_predict))\n",
        "print('Confusion Matrix:',confusion_matrix(y_val, y_predict))\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_val, y_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "print('AUC:', roc_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWaNgvd6d1wS"
      },
      "outputs": [],
      "source": [
        "f1_conf_int(y_val,y_predict) #gives f1 mean, ci lower, ci upper, std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2Ge-fned1wS"
      },
      "outputs": [],
      "source": [
        "rftfidf_f1 = pd.DataFrame(f1_conf_int(y_val, y_predict), index =list1,\n",
        "                                              columns =['val'])\n",
        "\n",
        "rftfidf_p = pd.DataFrame(precision_conf_int(y_val, y_predict), index =list1,\n",
        "                                              columns =['val'])\n",
        "\n",
        "rftfidf_r = pd.DataFrame(recall_conf_int(y_val, y_predict), index =list1,\n",
        "                                              columns =['val'])\n",
        "\n",
        "rf_tfidf_report = classification_report(y_val,y_predict, output_dict=True)\n",
        "rftfidf = pd.DataFrame(rf_tfidf_report).transpose()\n",
        "\n",
        "auc_list=[]\n",
        "\n",
        "auc_list.append(roc_auc)\n",
        "rftfidf_auc = pd.DataFrame(auc_list, columns=['auc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hw4fNLAxd1wS"
      },
      "outputs": [],
      "source": [
        "import _pickle as cPickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlcjydhvd1wS"
      },
      "outputs": [],
      "source": [
        "import _pickle as cPickle\n",
        "#import cPickle\n",
        "\n",
        "with open('RF_tfidf_may4', 'wb') as f:\n",
        "    cPickle.dump(rf_tfidf, f)\n",
        "\n",
        "\n",
        "# in your prediction file\n",
        "\n",
        "#with open('S:/AchlysShared/BRC_CRIS/Jaya Chaturvedi/13_Pain/from_gpu_vm_classifier_and_ampligraph/ampligraph/KNclf', 'rb') as f:\n",
        "#    kn_model = cPickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROGs3-jzd1wS"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "# save\n",
        "joblib.dump(rf_tfidf, \"RF_tfidf_may4.joblib\")\n",
        "\n",
        "# load\n",
        "#loaded_rf = joblib.load(\"S:/AchlysShared/BRC_CRIS/Jaya Chaturvedi/13_Pain/from_gpu_vm_classifier_and_ampligraph/ampligraph/randomforestclf.joblib\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSswJd01Df1r"
      },
      "outputs": [],
      "source": [
        "#FITTING THE CLASSIFICATION MODEL using Decision Tree (tf-idf) ---- try\n",
        "\n",
        "dt_tfidf = DecisionTreeClassifier()\n",
        "dt_tfidf.fit(X_train_vectors_tfidf, y_train)  #model\n",
        "\n",
        "#Predict y value for test dataset\n",
        "y_predict = dt_tfidf.predict(X_val_vectors_tfidf)\n",
        "y_prob = dt_tfidf.predict_proba(X_val_vectors_tfidf)[:,1]\n",
        "\n",
        "print('Decision Tree + tf-idf')\n",
        "print(classification_report(y_val,y_predict))\n",
        "print('Confusion Matrix:',confusion_matrix(y_val, y_predict))\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_val, y_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "print('AUC:', roc_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGK4_ZCud1wT"
      },
      "outputs": [],
      "source": [
        "f1_conf_int(y_val,y_predict) #gives f1 mean, ci lower, ci upper, std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHDc60Hgd1wT"
      },
      "outputs": [],
      "source": [
        "dttfidf_f1 = pd.DataFrame(f1_conf_int(y_val, y_predict), index =list1,\n",
        "                                              columns =['val'])\n",
        "\n",
        "dttfidf_p = pd.DataFrame(precision_conf_int(y_val, y_predict), index =list1,\n",
        "                                              columns =['val'])\n",
        "\n",
        "dttfidf_r = pd.DataFrame(recall_conf_int(y_val, y_predict), index =list1,\n",
        "                                              columns =['val'])\n",
        "\n",
        "dt_tfidf_report = classification_report(y_val,y_predict, output_dict=True)\n",
        "dttfidf = pd.DataFrame(dt_tfidf_report).transpose()\n",
        "\n",
        "auc_list=[]\n",
        "\n",
        "auc_list.append(roc_auc)\n",
        "dttfidf_auc = pd.DataFrame(auc_list, columns=['auc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Myq_SIkDwwN"
      },
      "outputs": [],
      "source": [
        "#FITTING THE CLASSIFICATION MODEL using SVC (tf-idf) ---- try\n",
        "\n",
        "svc_tfidf = SVC(probability=True)\n",
        "svc_tfidf.fit(X_train_vectors_tfidf, y_train)  #model\n",
        "\n",
        "#Predict y value for test dataset\n",
        "y_predict = svc_tfidf.predict(X_val_vectors_tfidf)\n",
        "y_prob = svc_tfidf.predict_proba(X_val_vectors_tfidf)[:,1]\n",
        "\n",
        "print('SVC + tf-idf')\n",
        "print(classification_report(y_val,y_predict))\n",
        "print('Confusion Matrix:',confusion_matrix(y_val, y_predict))\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_val, y_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "print('AUC:', roc_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLyNwsnud1wU"
      },
      "outputs": [],
      "source": [
        "svctfidf_f1 = pd.DataFrame(f1_conf_int(y_val, y_predict), index =list1,\n",
        "                                              columns =['val'])\n",
        "\n",
        "svctfidf_p = pd.DataFrame(precision_conf_int(y_val, y_predict), index =list1,\n",
        "                                              columns =['val'])\n",
        "\n",
        "svctfidf_r = pd.DataFrame(recall_conf_int(y_val, y_predict), index =list1,\n",
        "                                              columns =['val'])\n",
        "\n",
        "svc_tfidf_report = classification_report(y_val,y_predict, output_dict=True)\n",
        "svctfidf = pd.DataFrame(svc_tfidf_report).transpose()\n",
        "\n",
        "auc_list=[]\n",
        "\n",
        "auc_list.append(roc_auc)\n",
        "svctfidf_auc = pd.DataFrame(auc_list, columns=['auc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oj_7dHzTd1wU"
      },
      "outputs": [],
      "source": [
        "print(\"Model F1 Score: \", f1_score(y_val,y_predict))\n",
        "print(\"Model F1 mean, CI Lower, CI Upper, STD: \",f1_conf_int(y_val,y_predict))#gives f1 mean, ci lower, ci upper, std\n",
        "print(\"Model Precision: \", precision_score(y_val, y_predict))\n",
        "print(\"Model Precision mean, CI Lower, CI Upper, STD: \", precision_conf_int(y_val,y_predict))\n",
        "print(\"Model Recall: \", recall_score(y_val, y_predict))\n",
        "print(\"Model Recall mean, CI Lower, CI Upper, STD: \", recall_conf_int(y_val,y_predict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ITXx760D29n"
      },
      "outputs": [],
      "source": [
        "#FITTING THE CLASSIFICATION MODEL using Linear SVC (tf-idf) ---- try\n",
        "\n",
        "lsvc_tfidf = LinearSVC(multi_class='ovr', C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
        "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
        "      penalty='l2', random_state=0, tol=1e-05, verbose=0)\n",
        "lsvc_tfidf.fit(X_train_vectors_tfidf, y_train)  #model\n",
        "\n",
        "#Predict y value for test dataset\n",
        "y_predict = lsvc_tfidf.predict(X_val_vectors_tfidf)\n",
        "y_prob = lsvc_tfidf._predict_proba_lr(X_val_vectors_tfidf)[:,1]\n",
        "\n",
        "print('Linear SVC + tf-idf')\n",
        "print(classification_report(y_val,y_predict))\n",
        "print('Confusion Matrix:',confusion_matrix(y_val, y_predict))\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_val, y_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "print('AUC:', roc_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHZsG3jod1wU"
      },
      "outputs": [],
      "source": [
        "lsvctfidf_f1 = pd.DataFrame(f1_conf_int(y_val, y_predict), index =list1,\n",
        "                                              columns =['val'])\n",
        "\n",
        "lsvctfidf_p = pd.DataFrame(precision_conf_int(y_val, y_predict), index =list1,\n",
        "                                              columns =['val'])\n",
        "\n",
        "lsvctfidf_r = pd.DataFrame(recall_conf_int(y_val, y_predict), index =list1,\n",
        "                                              columns =['val'])\n",
        "\n",
        "lsvc_tfidf_report = classification_report(y_val,y_predict, output_dict=True)\n",
        "lsvctfidf = pd.DataFrame(lsvc_tfidf_report).transpose()\n",
        "\n",
        "auc_list=[]\n",
        "\n",
        "auc_list.append(roc_auc)\n",
        "lsvctfidf_auc = pd.DataFrame(auc_list, columns=['auc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJXPQs-gEAh3"
      },
      "outputs": [],
      "source": [
        "#FITTING THE CLASSIFICATION MODEL using SGD (tf-idf) ---- try\n",
        "\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "sgd_tfidf = SGDClassifier(loss='hinge',class_weight='balanced')\n",
        "clf = sgd_tfidf.fit(X_train_vectors_tfidf, y_train)  #model\n",
        "\n",
        "calibrator = CalibratedClassifierCV(clf, cv='prefit')\n",
        "model=calibrator.fit(X_train_vectors_tfidf, y_train)\n",
        "\n",
        "#Predict y value for test dataset\n",
        "y_predict = model.predict(X_val_vectors_tfidf)\n",
        "y_prob = model.predict_proba(X_val_vectors_tfidf)[:,1]\n",
        "\n",
        "print('SGD + tf-idf')\n",
        "print(classification_report(y_val,y_predict))\n",
        "print('Confusion Matrix:',confusion_matrix(y_val, y_predict))\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_val, y_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "print('AUC:', roc_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIajB8KSd1wV"
      },
      "outputs": [],
      "source": [
        "sgdtfidf_f1 = pd.DataFrame(f1_conf_int(y_val, y_predict), index =list1,\n",
        "                                              columns =['val'])\n",
        "\n",
        "sgdtfidf_p = pd.DataFrame(precision_conf_int(y_val, y_predict), index =list1,\n",
        "                                              columns =['val'])\n",
        "\n",
        "sgdtfidf_r = pd.DataFrame(recall_conf_int(y_val, y_predict), index =list1,\n",
        "                                              columns =['val'])\n",
        "\n",
        "sgd_tfidf_report = classification_report(y_val,y_predict, output_dict=True)\n",
        "sgdtfidf = pd.DataFrame(sgd_tfidf_report).transpose()\n",
        "\n",
        "auc_list=[]\n",
        "\n",
        "auc_list.append(roc_auc)\n",
        "sgdtfidf_auc = pd.DataFrame(auc_list, columns=['auc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-n1Zoi2EJrA"
      },
      "outputs": [],
      "source": [
        "#FITTING THE CLASSIFICATION MODEL using K neighbours (tf-idf) ---- try\n",
        "\n",
        "kn_tfidf = KNeighborsClassifier()\n",
        "kn = kn_tfidf.fit(X_train_vectors_tfidf, y_train)  #model\n",
        "\n",
        "calibrator = CalibratedClassifierCV(kn, cv='prefit')\n",
        "model=calibrator.fit(X_train_vectors_tfidf, y_train)\n",
        "\n",
        "#Predict y value for test dataset\n",
        "y_predict = model.predict(X_val_vectors_tfidf)\n",
        "y_prob = model.predict_proba(X_val_vectors_tfidf)[:,1]\n",
        "\n",
        "\n",
        "print('KNN + tf-idf')\n",
        "print(classification_report(y_val,y_predict))\n",
        "print('Confusion Matrix:',confusion_matrix(y_val, y_predict))\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_val, y_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "print('AUC:', roc_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d1wWeGhd1wV"
      },
      "outputs": [],
      "source": [
        "knntfidf_f1 = pd.DataFrame(f1_conf_int(y_val, y_predict), index =list1,\n",
        "                                              columns =['val'])\n",
        "\n",
        "knntfidf_p = pd.DataFrame(precision_conf_int(y_val, y_predict), index =list1,\n",
        "                                              columns =['val'])\n",
        "\n",
        "knntfidf_r = pd.DataFrame(recall_conf_int(y_val, y_predict), index =list1,\n",
        "                                              columns =['val'])\n",
        "\n",
        "knn_tfidf_report = classification_report(y_val,y_predict, output_dict=True)\n",
        "knntfidf = pd.DataFrame(knn_tfidf_report).transpose()\n",
        "\n",
        "auc_list=[]\n",
        "\n",
        "auc_list.append(roc_auc)\n",
        "knntfidf_auc = pd.DataFrame(auc_list, columns=['auc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjftxVa8d1wV"
      },
      "outputs": [],
      "source": [
        "#combine all classification reports\n",
        "frames=[lrtfidf,nbtfidf, rftfidf, dttfidf, svctfidf, lsvctfidf, sgdtfidf, knntfidf]\n",
        "result = pd.concat(frames, keys=[\"lr_tfidf\", \"nb_tfidf\", \"rf_tfidf\",\"dt_tfidf\",\"svc_tfidf\",\"lsvc_tfidf\",\"sgd_tfidf\",\"knn_tfidf\"])\n",
        "result.head(10)\n",
        "\n",
        "result.to_csv('./classification_reports.csv', index= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TajC9tSDd1wV"
      },
      "outputs": [],
      "source": [
        "auc_frames=[lrtfidf_auc,nbtfidf_auc, rftfidf_auc, dttfidf_auc, svctfidf_auc, lsvctfidf_auc, sgdtfidf_auc, knntfidf_auc]\n",
        "auc_result = pd.concat(auc_frames, keys=[\"lr_tfidf\", \"nb_tfidf\", \"rf_tfidf\",\"dt_tfidf\",\"svc_tfidf\",\"lsvc_tfidf\",\"sgd_tfidf\",\"knn_tfidf\"])\n",
        "auc_result.head(10)\n",
        "\n",
        "auc_result.to_csv('./AUC_reports.csv', index= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZgbipikd1wW"
      },
      "outputs": [],
      "source": [
        "f1_frames=[lrtfidf_f1,nbtfidf_f1, rftfidf_f1, dttfidf_f1, svctfidf_f1, lsvctfidf_f1, sgdtfidf_f1, knntfidf_f1]\n",
        "f1_result = pd.concat(f1_frames, keys=[\"lr_tfidf\", \"nb_tfidf\", \"rf_tfidf\",\"dt_tfidf\",\"svc_tfidf\",\"lsvc_tfidf\",\"sgd_tfidf\",\"knn_tfidf\"])\n",
        "f1_result.head(10)\n",
        "\n",
        "f1_result.to_csv('./f1_with_ci_reports.csv', index= True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhEESdNBd1wW"
      },
      "outputs": [],
      "source": [
        "p_frames=[lrtfidf_p,nbtfidf_p, rftfidf_p, dttfidf_p, svctfidf_p, lsvctfidf_p, sgdtfidf_p, knntfidf_p]\n",
        "p_result = pd.concat(p_frames, keys=[\"lr_tfidf\", \"nb_tfidf\", \"rf_tfidf\",\"dt_tfidf\",\"svc_tfidf\",\"lsvc_tfidf\",\"sgd_tfidf\",\"knn_tfidf\"])\n",
        "p_result.head(10)\n",
        "\n",
        "p_result.to_csv('./p_with_ci_reports.csv', index= True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ak9MeSBSd1wW"
      },
      "outputs": [],
      "source": [
        "r_frames=[lrtfidf_r,nbtfidf_r, rftfidf_r, dttfidf_r, svctfidf_r, lsvctfidf_r, sgdtfidf_r, knntfidf_r]\n",
        "r_result = pd.concat(r_frames, keys=[\"lr_tfidf\", \"nb_tfidf\", \"rf_tfidf\",\"dt_tfidf\",\"svc_tfidf\",\"lsvc_tfidf\",\"sgd_tfidf\",\"knn_tfidf\"])\n",
        "r_result.head(10)\n",
        "\n",
        "r_result.to_csv('./r_with_ci_reports.csv', index= True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTwpO3Ktd1wW"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOGNHxA3uKb8"
      },
      "outputs": [],
      "source": [
        "#FITTING THE CLASSIFICATION MODEL using Logistic Regression (W2v)\n",
        "lr_w2v=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')\n",
        "lr_w2v.fit(X_train_vectors_w2v, y_train)  #model\n",
        "\n",
        "#Predict y value for test dataset\n",
        "y_predict = lr_w2v.predict(X_val_vectors_w2v)\n",
        "y_prob = lr_w2v.predict_proba(X_val_vectors_w2v)[:,1]\n",
        "\n",
        "print('Logistic regression + w2v')\n",
        "print(classification_report(y_val,y_predict))\n",
        "print('Confusion Matrix:',confusion_matrix(y_val, y_predict))\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_val, y_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "print('AUC:', roc_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIJtPrQBd1wY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wG35vNupd1wY"
      },
      "source": [
        "## Run non BERT classifier on test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnCT66lgyD5x"
      },
      "outputs": [],
      "source": [
        "testing_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdFXS72yuKZn"
      },
      "outputs": [],
      "source": [
        "#Testing it on new dataset with the best model --- redo this with SVM classifier model\n",
        "#df_test=pd.read_csv('test.csv')  #reading the data\n",
        "df_test = testing_data\n",
        "df_test['clean_text'] = df_test['context'].apply(lambda x: finalpreprocess(x)) #preprocess the data\n",
        "X_test=df_test['clean_text']\n",
        "X_vector=tfidf_vectorizer.transform(X_test) #converting X_test to vector\n",
        "#y_predict = lr_tfidf.predict(X_vector)      #use the trained model on X_vector\n",
        "#y_predict = svc_tfidf.predict(X_vector)\n",
        "y_predict = rf_tfidf.predict(X_vector)\n",
        "#y_prob = lr_tfidf.predict_proba(X_vector)[:,1]\n",
        "#y_prob = svc_tfidf.predict_proba(X_vector)[:,1]\n",
        "y_prob = rf_tfidf.predict_proba(X_vector)[:,1]\n",
        "df_test['predict_prob']= y_prob\n",
        "df_test['label']= y_predict\n",
        "print(df_test.head())\n",
        "#final=df_test[['id','label']].reset_index(drop=True)\n",
        "df_test.to_csv(path+'rf_testing_label_pred.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1chiORNd1wb"
      },
      "outputs": [],
      "source": [
        "df_test = pd.read_csv('pain_sentences_for_classifier_160k_to_174k.csv')\n",
        "df_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrQn8H6Td1wb"
      },
      "outputs": [],
      "source": [
        "#Testing it on new dataset with the best model --- redo this with SVM classifier model\n",
        "#df_test=pd.read_csv('test.csv')  #reading the data\n",
        "#df_test = testing_data\n",
        "df_test['clean_text_2'] = df_test['clean_text'].apply(lambda x: finalpreprocess(x)) #preprocess the data\n",
        "X_test=df_test['clean_text_2']\n",
        "X_vector=tfidf_vectorizer.transform(X_test) #converting X_test to vector\n",
        "#y_predict = lr_tfidf.predict(X_vector)      #use the trained model on X_vector\n",
        "#y_predict = svc_tfidf.predict(X_vector)\n",
        "y_predict = rf_tfidf.predict(X_vector)\n",
        "#y_prob = lr_tfidf.predict_proba(X_vector)[:,1]\n",
        "#y_prob = svc_tfidf.predict_proba(X_vector)[:,1]\n",
        "y_prob = rf_tfidf.predict_proba(X_vector)[:,1]\n",
        "df_test['predict_prob']= y_prob\n",
        "df_test['label']= y_predict\n",
        "print(df_test.head())\n",
        "#final=df_test[['id','label']].reset_index(drop=True)\n",
        "df_test.to_csv(path+'rf_tfidf_160k_to_174k.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf2OTS8At-AY",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "# Other links to look at"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZYFlPr_d1wd"
      },
      "source": [
        "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.PrecisionRecallDisplay.html#sklearn.metrics.PrecisionRecallDisplay.from_estimator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfh2jwFad1we"
      },
      "source": [
        "https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#sphx-glr-auto-examples-model-selection-plot-precision-recall-py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhflNAzQd1we"
      },
      "source": [
        "https://machinelearningmastery.com/report-classifier-performance-confidence-intervals/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzM9oIEpd1we"
      },
      "source": [
        "https://stackoverflow.com/questions/66814430/seaborn-boxplot-with-confidence-intervals-for-the-mean?rq=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmcGSz9Rd1we"
      },
      "source": [
        "https://stackoverflow.com/questions/50062201/boxplot-in-pandas-with-confidence-intervals-and-bootstrap-returns-exception-re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exsLHiiid1wf"
      },
      "source": [
        "https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.boxplot.html#matplotlib.pyplot.boxplot  -- bootstrap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MEngaZkd1wf"
      },
      "source": [
        "https://machinelearningmastery.com/report-classifier-performance-confidence-intervals/ --formulas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5gCn5lDd1wf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "ug6mIzeH-Aq8",
        "6MekoG8e97cE",
        "-uZaA6PO90gm",
        "lCC2meSE9wdv",
        "MOzqD0ZN9qSt",
        "BNcnFOET9mKl"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}